<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222">


  <link rel="apple-touch-icon" sizes="180x180" href="/blog/uploads/images/gt.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/blog/uploads/images/gt-32.jpg">
  <link rel="icon" type="image/png" sizes="16x16" href="/blog/uploads/images/gt-16.jpg">
  <link rel="mask-icon" href="/blog/uploads/images/logo.png" color="#222">

<link rel="stylesheet" href="/blog/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css" integrity="sha256-DfWjNxDkM94fVBWx1H5BMMp0Zq7luBlV8QRcSES7s+0=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/themes/blue/pace-theme-minimal.css">
  <script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"github.com","root":"/blog/","images":"/blog/images","scheme":"Gemini","darkmode":false,"version":"8.11.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"},"path":"/blog/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/blog/js/config.js"></script>



<link rel="canonical" href="https://github.com/maxzhao-it/blog/page/30/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"en","comments":"","permalink":"","path":"page/30/index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>maxzhao</title>
  





  <noscript>
    <link rel="stylesheet" href="/blog/css/noscript.css">
  </noscript>
<link rel="alternate" href="/blog/atom.xml" title="maxzhao" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/blog/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">maxzhao</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">不要害怕Exception和Error</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/blog/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-about"><a href="/blog/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li><li class="menu-item menu-item-tags"><a href="/blog/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li><li class="menu-item menu-item-categories"><a href="/blog/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li><li class="menu-item menu-item-archives"><a href="/blog/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a></li><li class="menu-item menu-item-schedule"><a href="/blog/schedule/" rel="section"><i class="fa fa-calendar fa-fw"></i>Schedule</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="赵联胜"
      src="/blog/uploads/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">赵联胜</p>
  <div class="site-description" itemprop="description">小码农赵联胜的博客,从遇到Java到爱上Java，工作之余学习Java生态圈中的各种技术。</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/blog/archives/">
          <span class="site-state-item-count">443</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/blog/categories/">
        <span class="site-state-item-count">190</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/blog/tags/">
        <span class="site-state-item-count">240</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://gitee.com/zhaoliansheng" title="GitHub → https:&#x2F;&#x2F;gitee.com&#x2F;zhaoliansheng" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="/blog/1441439636@qq.com" title="E-Mail → 1441439636@qq.com"><i class="fa fa-envelope fa-fw"></i></a>
      </span>
  </div>


  <div class="links-of-blogroll site-overview-item animated">
    <div class="links-of-blogroll-title"><i class="fa fa-globe fa-fw"></i>
      友情链接
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://www.jianshu.com/u/e8c426a6007e" title="https:&#x2F;&#x2F;www.jianshu.com&#x2F;u&#x2F;e8c426a6007e" rel="noopener" target="_blank">我的简书</a>
        </li>
    </ul>
  </div>

        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://github.com/maxzhao-it/blog/post/1504/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/blog/uploads/images/avatar.jpg">
      <meta itemprop="name" content="赵联胜">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="maxzhao">
      <meta itemprop="description" content="小码农赵联胜的博客,从遇到Java到爱上Java，工作之余学习Java生态圈中的各种技术。">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | maxzhao">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/blog/post/1504/" class="post-title-link" itemprop="url">POI设置单元格下拉列表（级联列表、自动填充）</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2022-07-28 17:39:49" itemprop="dateModified" datetime="2022-07-28T17:39:49+08:00">2022-07-28</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/blog/categories/Java/" itemprop="url" rel="index"><span itemprop="name">Java</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/blog/categories/Java/util/" itemprop="url" rel="index"><span itemprop="name">util</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/blog/categories/Java/util/POI/" itemprop="url" rel="index"><span itemprop="name">POI</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <hr>
<ol>
<li>单独下拉列表</li>
<li>级联列表</li>
<li>自动填充</li>
</ol>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.poi.hssf.usermodel.DVConstraint;</span><br><span class="line"><span class="keyword">import</span> org.apache.poi.hssf.usermodel.HSSFDataValidation;</span><br><span class="line"><span class="keyword">import</span> org.apache.poi.hssf.usermodel.HSSFWorkbook;</span><br><span class="line"><span class="keyword">import</span> org.apache.poi.ss.usermodel.*;</span><br><span class="line"><span class="keyword">import</span> org.apache.poi.ss.util.CellRangeAddressList;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.util.*;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * ExcelUtil</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> maxzhao</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@date</span> 2021-03-24 14:51</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">BootExcelUtil</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 给sheet页，添加下拉列表</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> workbook    excel文件，用于添加Name</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> targetSheet 级联列表所在sheet页</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> options     级联数据 [&#x27;百度&#x27;,&#x27;阿里巴巴&#x27;]</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> column      下拉列表所在列 从&#x27;A&#x27;开始</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> fromRow     下拉限制开始行</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> endRow      下拉限制结束行</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">addValidationToSheet</span><span class="params">(Workbook workbook, Sheet targetSheet, Object[] options, <span class="type">char</span> column, <span class="type">int</span> fromRow, <span class="type">int</span> endRow)</span> &#123;</span><br><span class="line">        <span class="type">String</span> <span class="variable">hiddenSheetName</span> <span class="operator">=</span> <span class="string">&quot;sheet&quot;</span> + workbook.getNumberOfSheets();</span><br><span class="line">        <span class="type">Sheet</span> <span class="variable">optionsSheet</span> <span class="operator">=</span> workbook.createSheet(hiddenSheetName);</span><br><span class="line">        <span class="type">String</span> <span class="variable">nameName</span> <span class="operator">=</span> column + <span class="string">&quot;_parent&quot;</span>;</span><br><span class="line">        <span class="type">int</span> <span class="variable">rowIndex</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">        Cell cellTemp;</span><br><span class="line">        Row rowTemp;</span><br><span class="line">        <span class="type">int</span> <span class="variable">columnIndex</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (Object option : options) &#123;</span><br><span class="line">            columnIndex = <span class="number">0</span>;</span><br><span class="line">            rowTemp = optionsSheet.createRow(rowIndex++);</span><br><span class="line">            cellTemp = rowTemp.createCell(columnIndex++);</span><br><span class="line">            cellTemp.setCellValue(option.toString());</span><br><span class="line">        &#125;</span><br><span class="line">        createName(workbook, nameName, hiddenSheetName + <span class="string">&quot;!$A$1:$A$&quot;</span> + options.length);</span><br><span class="line">        <span class="type">DVConstraint</span> <span class="variable">constraint</span> <span class="operator">=</span> DVConstraint.createFormulaListConstraint(nameName);</span><br><span class="line">        <span class="type">CellRangeAddressList</span> <span class="variable">regions</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">CellRangeAddressList</span>(fromRow, endRow, (<span class="type">int</span>) column - <span class="string">&#x27;A&#x27;</span>, (<span class="type">int</span>) column - <span class="string">&#x27;A&#x27;</span>);</span><br><span class="line">        targetSheet.addValidationData(<span class="keyword">new</span> <span class="title class_">HSSFDataValidation</span>(regions, constraint));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 给sheet页  添加级联下拉列表</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> workbook    excel</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> targetSheet sheet页</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> options     要添加的下拉列表内容  ， keys 是下拉列表1中的内容，每个Map.Entry.Value 是对应的级联下拉列表内容</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> keyColumn   下拉列表1位置</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> valueColumn 级联下拉列表位置</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> fromRow     级联限制开始行</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> endRow      级联限制结束行</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">addValidationToSheet</span><span class="params">(Workbook workbook, Sheet targetSheet, Map&lt;String, List&lt;String&gt;&gt; options, <span class="type">char</span> keyColumn, <span class="type">char</span> valueColumn, <span class="type">int</span> fromRow, <span class="type">int</span> endRow)</span> &#123;</span><br><span class="line">        <span class="type">String</span> <span class="variable">hiddenSheetName</span> <span class="operator">=</span> <span class="string">&quot;sheet&quot;</span> + workbook.getNumberOfSheets();</span><br><span class="line">        <span class="type">Sheet</span> <span class="variable">hiddenSheet</span> <span class="operator">=</span> workbook.createSheet(hiddenSheetName);</span><br><span class="line">        List&lt;String&gt; firstLevelItems = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line"></span><br><span class="line">        <span class="type">int</span> <span class="variable">rowIndex</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">        DVConstraint constraintTemp;</span><br><span class="line">        Cell cellTemp;</span><br><span class="line">        Row rowTemp;</span><br><span class="line">        String parentTemp;</span><br><span class="line">        <span class="keyword">for</span> (Map.Entry&lt;String, List&lt;String&gt;&gt; entry : options.entrySet()) &#123;</span><br><span class="line">            parentTemp = formatNameName(entry.getKey());</span><br><span class="line">            firstLevelItems.add(parentTemp);</span><br><span class="line">            List&lt;String&gt; children = entry.getValue();</span><br><span class="line"></span><br><span class="line">            <span class="type">int</span> <span class="variable">columnIndex</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">            rowTemp = hiddenSheet.createRow(rowIndex++);</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> (String child : children) &#123;</span><br><span class="line">                cellTemp = rowTemp.createCell(columnIndex++);</span><br><span class="line">                cellTemp.setCellValue(child);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="type">char</span> <span class="variable">lastChildrenColumn</span> <span class="operator">=</span> (<span class="type">char</span>) ((<span class="type">int</span>) <span class="string">&#x27;A&#x27;</span> + children.size() - <span class="number">1</span>);</span><br><span class="line">            createName(workbook, parentTemp, String.format(hiddenSheetName + <span class="string">&quot;!$A$%s:$%s$%s&quot;</span>, rowIndex, lastChildrenColumn, rowIndex));</span><br><span class="line">            constraintTemp = DVConstraint.createFormulaListConstraint(<span class="string">&quot;INDIRECT($&quot;</span> + keyColumn + <span class="string">&quot;1)&quot;</span>);</span><br><span class="line">            <span class="type">CellRangeAddressList</span> <span class="variable">regions</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">CellRangeAddressList</span>(fromRow, endRow, valueColumn - <span class="string">&#x27;A&#x27;</span>, valueColumn - <span class="string">&#x27;A&#x27;</span>);</span><br><span class="line">            targetSheet.addValidationData(<span class="keyword">new</span> <span class="title class_">HSSFDataValidation</span>(regions, constraintTemp));</span><br><span class="line">        &#125;</span><br><span class="line">        addValidationToSheet(workbook, targetSheet, firstLevelItems.toArray(), keyColumn, fromRow, endRow);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 根据用户在keyColumn选择的key, 自动填充value到valueColumn</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> workbook    excel</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> targetSheet sheet页</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> keyValues   匹配关系 &#123;&#x27;百度&#x27;,&#x27;www.baidu.com&#x27;&#125;,&#123;&#x27;淘宝&#x27;,&#x27;www.taobao.com&#x27;&#125;</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> keyColumn  自动填充 要匹配的列（例如 网站中文名称）</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> valueColumn 匹配到的内容列（例如 网址）</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> fromRow     下拉限制开始行</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> endRow      下拉限制结束行</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">addAutoMatchValidationToSheet</span><span class="params">(Workbook workbook, Sheet targetSheet, Map&lt;String, String&gt; keyValues, <span class="type">char</span> keyColumn, <span class="type">char</span> valueColumn, <span class="type">int</span> fromRow, <span class="type">int</span> endRow)</span> &#123;</span><br><span class="line">        <span class="type">String</span> <span class="variable">hiddenSheetName</span> <span class="operator">=</span> <span class="string">&quot;sheet&quot;</span> + workbook.getNumberOfSheets();</span><br><span class="line">        <span class="type">Sheet</span> <span class="variable">hiddenSheet</span> <span class="operator">=</span> workbook.createSheet(hiddenSheetName);</span><br><span class="line"></span><br><span class="line">        Cell cellTemp;</span><br><span class="line">        Row rowTemp;</span><br><span class="line">        <span class="comment">// init the search region(A and B columns in hiddenSheet)</span></span><br><span class="line">        <span class="type">int</span> <span class="variable">rowIndex</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (Map.Entry&lt;String, String&gt; kv : keyValues.entrySet()) &#123;</span><br><span class="line">            rowTemp = hiddenSheet.createRow(rowIndex++);</span><br><span class="line"></span><br><span class="line">            cellTemp = rowTemp.createCell(<span class="number">0</span>);</span><br><span class="line">            cellTemp.setCellValue(kv.getKey());</span><br><span class="line"></span><br><span class="line">            cellTemp = rowTemp.createCell(<span class="number">1</span>);</span><br><span class="line">            cellTemp.setCellValue(kv.getValue());</span><br><span class="line">        &#125;</span><br><span class="line">        String keyCellTemp;</span><br><span class="line">        String formulaTemp;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> fromRow; i &lt;= endRow; i++) &#123;</span><br><span class="line">            <span class="type">Row</span> <span class="variable">totalSheetRow</span> <span class="operator">=</span> targetSheet.getRow(i);</span><br><span class="line">            <span class="keyword">if</span> (totalSheetRow == <span class="literal">null</span>) &#123;</span><br><span class="line">                totalSheetRow = targetSheet.createRow(i);</span><br><span class="line">            &#125;</span><br><span class="line">            cellTemp = totalSheetRow.getCell((<span class="type">int</span>) valueColumn - <span class="string">&#x27;A&#x27;</span>);</span><br><span class="line">            <span class="keyword">if</span> (cellTemp == <span class="literal">null</span>) &#123;</span><br><span class="line">                cellTemp = totalSheetRow.createCell((<span class="type">int</span>) valueColumn - <span class="string">&#x27;A&#x27;</span>);</span><br><span class="line">            &#125;</span><br><span class="line">            keyCellTemp = String.valueOf(keyColumn) + (i + <span class="number">1</span>);</span><br><span class="line">            formulaTemp = String.format(<span class="string">&quot;IF(ISNA(VLOOKUP(%s,%s!A:B,2,0)),\&quot;\&quot;,VLOOKUP(%s,%s!A:B,2,0))&quot;</span>, keyCellTemp, hiddenSheetName, keyCellTemp, hiddenSheetName);</span><br><span class="line">            cellTemp.setCellFormula(formulaTemp);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// init the keyColumn as comboList</span></span><br><span class="line">        addValidationToSheet(workbook, targetSheet, keyValues.keySet().toArray(), keyColumn, fromRow, endRow);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> Name <span class="title function_">createName</span><span class="params">(Workbook workbook, String nameName, String formula)</span> &#123;</span><br><span class="line">        <span class="type">Name</span> <span class="variable">name</span> <span class="operator">=</span> workbook.createName();</span><br><span class="line">        name.setNameName(nameName);</span><br><span class="line">        name.setRefersToFormula(formula);</span><br><span class="line">        <span class="keyword">return</span> name;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 隐藏excel中的sheet页</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> workbook</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> start    需要隐藏的 sheet开始索引</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">hideTempDataSheet</span><span class="params">(HSSFWorkbook workbook, <span class="type">int</span> start)</span> &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> start; i &lt; workbook.getNumberOfSheets(); i++) &#123;</span><br><span class="line">            workbook.setSheetHidden(i, <span class="literal">true</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 不可数字开头</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> name</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">static</span> String <span class="title function_">formatNameName</span><span class="params">(String name)</span> &#123;</span><br><span class="line">        name = name.replaceAll(<span class="string">&quot; &quot;</span>, <span class="string">&quot;&quot;</span>).replaceAll(<span class="string">&quot;-&quot;</span>, <span class="string">&quot;_&quot;</span>).replaceAll(<span class="string">&quot;:&quot;</span>, <span class="string">&quot;.&quot;</span>);</span><br><span class="line">        <span class="keyword">if</span> (Character.isDigit(name.charAt(<span class="number">0</span>))) &#123;</span><br><span class="line">            name = <span class="string">&quot;_&quot;</span> + name;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> name;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * poi</span></span><br><span class="line"><span class="comment">     * 测试</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> args</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> IOExceptionjava</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//demo 单独下拉列表</span></span><br><span class="line">        <span class="comment">//BootExcelUtil.addValidationToSheet(workbook, sheet, new String[]&#123;&quot;百度&quot;, &quot;阿里巴巴&quot;&#125;, &#x27;C&#x27;, 1, 200);</span></span><br><span class="line">        <span class="comment">//demo 级联下拉列表，一级不需要设置单独下拉</span></span><br><span class="line">        Map&lt;String, List&lt;String&gt;&gt; data = <span class="keyword">new</span> <span class="title class_">HashMap</span>&lt;&gt;();</span><br><span class="line">        data.put(<span class="string">&quot;百度系列&quot;</span>, Arrays.asList(<span class="string">&quot;百度地图&quot;</span>, <span class="string">&quot;百度知道&quot;</span>, <span class="string">&quot;百度音乐&quot;</span>));</span><br><span class="line">        data.put(<span class="string">&quot;阿里系列&quot;</span>, Arrays.asList(<span class="string">&quot;淘宝&quot;</span>, <span class="string">&quot;支付宝&quot;</span>, <span class="string">&quot;钉钉&quot;</span>));</span><br><span class="line">        <span class="comment">//BootExcelUtil.addValidationToSheet(workbook, sheet, data, &#x27;A&#x27;, &#x27;B&#x27;, 1, 200);</span></span><br><span class="line">        <span class="comment">//demo 自动填充</span></span><br><span class="line">        Map&lt;String, String&gt; kvs = <span class="keyword">new</span> <span class="title class_">HashMap</span>&lt;&gt;();</span><br><span class="line">        kvs.put(<span class="string">&quot;百度&quot;</span>, <span class="string">&quot;www.baidu.com&quot;</span>);</span><br><span class="line">        kvs.put(<span class="string">&quot;阿里&quot;</span>, <span class="string">&quot;www.taobao.com&quot;</span>);</span><br><span class="line">        <span class="comment">//BootExcelUtil.addAutoMatchValidationToSheet(workbook, sheet, kvs, &#x27;D&#x27;, &#x27;E&#x27;, 1, 200);</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>本文地址： <a href="https://github.com/maxzhao-it/blog/post/1504/">https://github.com/maxzhao-it/blog/post/1504/</a> </p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://github.com/maxzhao-it/blog/post/26523/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/blog/uploads/images/avatar.jpg">
      <meta itemprop="name" content="赵联胜">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="maxzhao">
      <meta itemprop="description" content="小码农赵联胜的博客,从遇到Java到爱上Java，工作之余学习Java生态圈中的各种技术。">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | maxzhao">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/blog/post/26523/" class="post-title-link" itemprop="url">pacman忽略软件包的升级</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2022-07-04 14:50:22" itemprop="dateModified" datetime="2022-07-04T14:50:22+08:00">2022-07-04</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/blog/categories/Linux/" itemprop="url" rel="index"><span itemprop="name">Linux</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/blog/categories/Linux/ArchLinux/" itemprop="url" rel="index"><span itemprop="name">ArchLinux</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p><code>pacman</code>升级时，有不少软件不需要升级或是升级后不稳定，可以用下面方式忽略升级</p>
<h3 id="忽略升级"><a href="#忽略升级" class="headerlink" title="忽略升级"></a>忽略升级</h3><p><code>/etc/pacman.conf</code> 中查找 <code>＃IgnorePkg　=</code></p>
<p>后面添加不想升级的软件包</p>
<ol>
<li><code>IgnorePkg</code> 软件包名称</li>
<li><code>IgnoreGroup</code>  软件包组名称</li>
</ol>
<h3 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">pacman -Sy abc         #和源同步后安装名为abc的包</span><br><span class="line">pacman -S  abc         #从本地数据库中得到abc的信息，下载安装abc包</span><br><span class="line">pacman -Sf abc         #强制安装包abc</span><br><span class="line">pacman -Ss abc         #搜索有关abc信息的包</span><br><span class="line">pacman -Si abc         #从数据库中搜索包abc的信息</span><br><span class="line">pacman -Q             # 列出已经安装的软件包</span><br><span class="line">pacman -Q abc         # 检查 abc 软件包是否已经安装</span><br><span class="line">pacman -Qi abc         #列出已安装的包abc的详细信息</span><br><span class="line">pacman -Ql abc         # 列出abc软件包的所有文件</span><br><span class="line">pacman -Qo /path/to/abc # 列出abc文件所属的软件包</span><br><span class="line">pacman -Syu           #同步源，并更新系统</span><br><span class="line">pacman -Sy            #仅同步源</span><br><span class="line">pacman -Su            #更新系统</span><br><span class="line">pacman -R  abc         #删除abc包</span><br><span class="line">pacman -Rd abc        #强制删除被依赖的包</span><br><span class="line">pacman -Rc abc         #删除abc包和依赖abc的包</span><br><span class="line">pacman -Rsc abc        #删除abc包和abc依赖的包</span><br><span class="line">pacman -Sc            #清理/var/cache/pacman/pkg目录下的旧包</span><br><span class="line">pacman -Scc           #清除所有下载的包和数据库</span><br><span class="line">pacman -U  abc         #安装下载的abs包，或新编译的abc包</span><br><span class="line">pacman -Sd abc         #忽略依赖性问题，安装包abc</span><br><span class="line">pacman -Su --ignore foo   #升级时不升级包foo</span><br><span class="line">pacman -Sg abc         #查询abc这个包组包含的软件包</span><br></pre></td></tr></table></figure>



<p>本文地址： <a href="https://github.com/maxzhao-it/blog/post/26523/">https://github.com/maxzhao-it/blog/post/26523/</a> </p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://github.com/maxzhao-it/blog/post/47598/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/blog/uploads/images/avatar.jpg">
      <meta itemprop="name" content="赵联胜">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="maxzhao">
      <meta itemprop="description" content="小码农赵联胜的博客,从遇到Java到爱上Java，工作之余学习Java生态圈中的各种技术。">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | maxzhao">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/blog/post/47598/" class="post-title-link" itemprop="url">Docker安装Greenplum集群</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2022-07-28 17:41:37" itemprop="dateModified" datetime="2022-07-28T17:41:37+08:00">2022-07-28</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/blog/categories/DB/" itemprop="url" rel="index"><span itemprop="name">DB</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/blog/categories/DB/PostgreSql/" itemprop="url" rel="index"><span itemprop="name">PostgreSql</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/blog/categories/DB/PostgreSql/Greenplum/" itemprop="url" rel="index"><span itemprop="name">Greenplum</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/blog/categories/DB/PostgreSql/Greenplum/%E9%9B%86%E7%BE%A4/" itemprop="url" rel="index"><span itemprop="name">集群</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <blockquote>
<p><code>Docker</code>安装可以参考：</p>
<p><a href="../22928/">Manjaro安装Docker</a></p>
<p><a href="../21945/">Centos7 安装 docker</a></p>
</blockquote>
<h3 id="一、创建-docker-节点"><a href="#一、创建-docker-节点" class="headerlink" title="一、创建 docker 节点"></a>一、创建 <code>docker</code> 节点</h3><h4 id="拉取centos镜像"><a href="#拉取centos镜像" class="headerlink" title="拉取centos镜像"></a>拉取centos镜像</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker pull centos</span><br></pre></td></tr></table></figure>

<p>这里如果没有权限，则参考 <a href="../22928/">Manjaro安装Docker</a> 中的分组操作。</p>
<h4 id="查看镜像"><a href="#查看镜像" class="headerlink" title="查看镜像"></a>查看镜像</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker images</span><br></pre></td></tr></table></figure>

<h4 id="创建几个容器，作为greenplum的节点"><a href="#创建几个容器，作为greenplum的节点" class="headerlink" title="创建几个容器，作为greenplum的节点"></a>创建几个容器，作为<code>greenplum</code>的节点</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 用 exit退出</span></span><br><span class="line">docker run -it --name gp-master centos /bin/bash</span><br><span class="line">docker run -it --name gp-master1 centos /bin/bash</span><br><span class="line">docker run -it --name gp-master2 centos /bin/bash</span><br><span class="line">docker run -it --name gp-master3 centos /bin/bash</span><br></pre></td></tr></table></figure>

<blockquote>
<p>注意：<code>exit</code> 退出后，容器停止(<code>Exited</code>)</p>
<p>命令解释：为<code>centos</code>这个镜像创建一个容器</p>
<ul>
<li><code>run</code>: 在新的容器中运行命令 . <code>run</code>&#x3D;<code>create</code> + <code>start</code></li>
<li><code>-it</code> : <code>-i</code>和<code>-t</code>, 为该<code>docker</code>创建一个伪终端，这样就可以进入到容器的交互模式</li>
<li><code>--name</code>  <code>gp-master</code> 容器名称</li>
<li><code>centos</code>： 镜像</li>
<li><code>*/bin/bash</code> ：表示启动容器后启动<code>bash</code>。<code>docker</code>中必须要保持一个进程的运行，要不然整个容器启动后就会马上<code>kill itself</code></li>
</ul>
<p>查看帮助：<code>docker run --help</code></p>
<p>守护态运行，通过 <code>run</code>后加<code>-d</code>实现</p>
</blockquote>
<h3 id="二、配置基础环境"><a href="#二、配置基础环境" class="headerlink" title="二、配置基础环境"></a>二、配置基础环境</h3><p>进入每个<code>greenplum</code>节点，配置基础环境</p>
<p>由于<code>docker</code>的<code>centos</code>镜像是<code>centos</code>的简化版本，里面有很多包是没有安装的，会影响到后面部署<code>greenplum</code>，因此在<code>docker</code>的每个节点中安装相关的依赖包</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看正在运行的容器</span></span><br><span class="line">docker ps -s</span><br><span class="line"><span class="comment"># 查看所有容器</span></span><br><span class="line">docker ps -a</span><br><span class="line"><span class="comment"># 启动</span></span><br><span class="line">docker start gp-master</span><br><span class="line">docker start gp-master1</span><br><span class="line">docker start gp-master2</span><br><span class="line">docker start gp-master3</span><br><span class="line"><span class="comment"># 停止容器</span></span><br><span class="line"><span class="comment"># docker stop gp-master</span></span><br><span class="line"><span class="comment"># 进入容器</span></span><br><span class="line">docker <span class="built_in">exec</span> -it gp-master /bin/bash</span><br></pre></td></tr></table></figure>

<ul>
<li>安装相关的依赖包</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install -y net-tools <span class="built_in">which</span> openssh-clients openssh-server less zip unzip iproute</span><br></pre></td></tr></table></figure>

<ul>
<li>启动ssh</li>
</ul>
<p>docker中默认没有启动ssh，为了方便各节点之间的互连，创建相关的认证key，并启动docker的每个节点里面的ssh</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa -f /etc/ssh/ssh_host_rsa_key</span><br><span class="line">ssh-keygen -t ecdsa -f /etc/ssh/ssh_host_ecdsa_key</span><br><span class="line">ssh-keygen -t ed25519 -f /etc/ssh/ssh_host_ed25519_key</span><br><span class="line">/usr/sbin/sshd</span><br></pre></td></tr></table></figure>

<ul>
<li>修改&#x2F;etc&#x2F;hosts文件</li>
</ul>
<p>在每个<code>docker</code>节点中添加如下配置，方便后续<code>greenplum</code>集群的配置文件中用到，<code>ip</code>为各个<code>docker</code>节点中的ip地址</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看</span></span><br><span class="line"><span class="built_in">cat</span> /etc/hosts</span><br><span class="line"><span class="comment"># 写入</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;172.17.0.2 dw-greenplum-1 mdw&quot;</span> &gt;&gt; /etc/hosts</span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;172.17.0.3 dw-greenplum-2 sdw1&quot;</span> &gt;&gt; /etc/hosts</span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;172.17.0.4 dw-greenplum-3 sdw2&quot;</span> &gt;&gt; /etc/hosts</span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;172.17.0.5 dw-greenplum-4 sdw3&quot;</span> &gt;&gt; /etc/hosts</span><br></pre></td></tr></table></figure>

<p>同时修改所有节点里面的<code>/etc/sysconfig/network</code>文件，保持与主机名一致</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看</span></span><br><span class="line"><span class="built_in">cat</span> /etc/sysconfig/network</span><br><span class="line"><span class="comment"># 不存在,则写入</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;NETWORKING=yes&quot;</span> &gt;&gt; /etc/sysconfig/network</span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;HOSTNAME=mdw&quot;</span> &gt;&gt; /etc/sysconfig/network</span><br><span class="line"><span class="comment"># 存在,则修改</span></span><br><span class="line">vi /etc/sysconfig/network</span><br><span class="line">NETWORKING=<span class="built_in">yes</span></span><br><span class="line">HOSTNAME=mdw</span><br></pre></td></tr></table></figure>

<ul>
<li>创建greenplum的用户和用户组</li>
</ul>
<p>为了方便安装<code>greenplum</code>集群，且使<code>greenplum</code>自带的<code>python</code>不与系统的<code>python</code>版本相冲突，在每个节点中创建<code>greenplum</code>的用户和用户组</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">groupadd -g 530 gpadmin</span><br><span class="line">useradd -g 530 -u 530 -m -d /home/gpadmin -s /bin/bash gpadmin</span><br><span class="line"><span class="built_in">chown</span> -R gpadmin:gpadmin /home/gpadmin</span><br><span class="line"><span class="comment"># 这里可能会提示 bash: passwd: command not found</span></span><br><span class="line">passwd gpadmin</span><br></pre></td></tr></table></figure>

<ul>
<li>修改每个节点上的文件打开数量限制</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cat</span> /etc/security/limits.conf</span><br><span class="line"><span class="comment"># 不存在,则写入, 存在,则用 vi 修改</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;soft nofile 65536&quot;</span> &gt;&gt; /etc/security/limits.conf</span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;hard nofile 65536&quot;</span> &gt;&gt; /etc/security/limits.conf</span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;soft nproc 131072&quot;</span> &gt;&gt; /etc/security/limits.conf</span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;hard nproc 131072&quot;</span> &gt;&gt; /etc/security/limits.conf</span><br></pre></td></tr></table></figure>

<ul>
<li>关闭每个节点上的防火墙，关闭selinux</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 最新的centos好像这些都不需要操作</span></span><br><span class="line">service iptables stop</span><br><span class="line">chkconfig iptables off</span><br><span class="line">vi /etc/selinux/config </span><br><span class="line"><span class="comment"># This file controls the state of SELinux on the system.</span></span><br><span class="line"><span class="comment"># SELINUX= can take one of these three values:</span></span><br><span class="line"><span class="comment">#     enforcing - SELinux security policy is enforced.</span></span><br><span class="line"><span class="comment">#     permissive - SELinux prints warnings instead of enforcing.</span></span><br><span class="line"><span class="comment">#     disabled - No SELinux policy is loaded.</span></span><br><span class="line">SELINUX=disabled</span><br><span class="line"><span class="comment"># SELINUXTYPE= can take one of these two values:</span></span><br><span class="line"><span class="comment">#     targeted - Targeted processes are protected,</span></span><br><span class="line"><span class="comment">#     mls - Multi Level Security protection.</span></span><br><span class="line">SELINUXTYPE=targeted </span><br></pre></td></tr></table></figure>

<h3 id="三、下载greenplum安装包"><a href="#三、下载greenplum安装包" class="headerlink" title="三、下载greenplum安装包"></a>三、下载<code>greenplum</code>安装包</h3><p>到<code>greenplum</code>的官网上，下载<a target="_blank" rel="noopener" href="https://network.pivotal.io/products/pivotal-gpdb">greenplum安装包</a><br>或<a href="https://github.com/greenplum-db/gpdb/releases">GitHub下载</a>，点开<code>Greenplum Database Server</code>，根据自己的操作系统下载安装包，我下载当前最新的<code> greenplum-db-6.15.0-rhel7-x86_64.rpm</code>，将其拷到<code>master</code>节点的<code>/home/gpadmin</code>目录中</p>
<blockquote>
</blockquote>
<p>版本快速链接: <a href="https://github.com/greenplum-db/gpdb/releases/download/6.15.0/open-source-greenplum-db-6.15.0-rhel7-x86_64.rpm">github:open-source-greenplum-db-6.15.0-rhel7-x86_64.rpm</a></p>
<h3 id="四、在master节点上安装greenplum"><a href="#四、在master节点上安装greenplum" class="headerlink" title="四、在master节点上安装greenplum"></a>四、在<code>master</code>节点上安装<code>greenplum</code></h3><p>切换到gpadmin用户</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">su gpadmin</span><br><span class="line"><span class="built_in">cd</span> ~/</span><br></pre></td></tr></table></figure>

<h4 id="传递文件到docker"><a href="#传递文件到docker" class="headerlink" title="传递文件到docker"></a>传递文件到<code>docker</code></h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker ps </span><br><span class="line">docker <span class="built_in">cp</span> /home/maxzhao/Downloads/open-source-greenplum-db-6.15.0-rhel7-x86_64.rpm 348d61c65324:/home/gpadmin/open-source-greenplum-db-6.15.0-rhel7-x86_64.rpm</span><br></pre></td></tr></table></figure>

<h4 id="Rpm-安装"><a href="#Rpm-安装" class="headerlink" title="Rpm 安装"></a><code>Rpm</code> 安装</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum localinstall /home/gpadmin/open-source-greenplum-db-6.15.0-rhel7-x86_64.rpm -y</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看安装地址</span></span><br><span class="line">[root@3b2ae1fbe58b /]<span class="comment"># whereis greenplum-db</span></span><br><span class="line">greenplum-db: /usr/local/greenplum-db</span><br><span class="line">[root@3b2ae1fbe58b /]<span class="comment"># ls -l /usr/local/greenplum-db </span></span><br><span class="line">lrwxrwxrwx 1 root root 30 Apr 20 11:14 /usr/local/greenplum-db -&gt; /usr/local/greenplum-db-6.15.0</span><br></pre></td></tr></table></figure>

<blockquote>
<p><code>gpssh-exkeys</code> 报错问题:</p>
<p>cd &#x2F;usr&#x2F;bin</p>
<p>mv python python.bak</p>
</blockquote>
<h4 id="解压安装-ZIP"><a href="#解压安装-ZIP" class="headerlink" title="解压安装(ZIP)"></a>解压安装(ZIP)</h4><p>解压下载后的zip文件</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">unzip greenplum-db-5.10.2-rhel7-x86_64.zip</span><br></pre></td></tr></table></figure>

<p>执行安装文件</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./greenplum-db-5.10.2-rhel7-x86_64.bin</span><br></pre></td></tr></table></figure>

<p>安装期间需要配置安装目录，输入<code>/home/gpadmin/greenplum-db-5.10.2</code></p>
<h4 id="为了方便安装集群，greenplum提供了批量操作节点的命令，通过指定配置文件使用批处理命令"><a href="#为了方便安装集群，greenplum提供了批量操作节点的命令，通过指定配置文件使用批处理命令" class="headerlink" title="为了方便安装集群，greenplum提供了批量操作节点的命令，通过指定配置文件使用批处理命令"></a>为了方便安装集群，<code>greenplum</code>提供了批量操作节点的命令，通过指定配置文件使用批处理命令</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> ~/ ;<span class="built_in">mkdir</span> conf</span><br><span class="line"><span class="built_in">echo</span> mdw &gt; ./conf/hostlist</span><br><span class="line"><span class="built_in">echo</span> sdw1 &gt;&gt; ./conf/hostlist</span><br><span class="line"><span class="built_in">echo</span> sdw2 &gt;&gt; ./conf/hostlist</span><br><span class="line"><span class="built_in">echo</span> sdw3 &gt;&gt; ./conf/hostlist</span><br><span class="line"><span class="built_in">cat</span> ./conf/hostlist</span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> sdw1 &gt; ./conf/seg_hosts </span><br><span class="line"><span class="built_in">echo</span> sdw2 &gt;&gt; ./conf/seg_hosts </span><br><span class="line"><span class="built_in">echo</span> sdw3 &gt;&gt; ./conf/seg_hosts </span><br><span class="line"><span class="built_in">cat</span> ./conf/seg_hosts</span><br></pre></td></tr></table></figure>

<p><code>greenplum-db/greenplum_path.sh</code>中保存了运行<code>greenplum</code>的一些环境变量，包括GPHOME、PYTHONHOME等，在<code>gpadmin</code>账号下设置环境变量，并将master节点的<code>key</code><br>交换到各个<code>segment</code>节点</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">[gpadmin@mdw ~]$ <span class="built_in">source</span> /home/gpadmin/greenplum-db/greenplum_path.sh </span><br><span class="line"><span class="comment"># 或者</span></span><br><span class="line">[gpadmin@mdw ~]$ <span class="built_in">source</span> /usr/local/greenplum-db/greenplum_path.sh </span><br><span class="line">[gpadmin@mdw ~]$ gpssh-exkeys -f /home/gpadmin/conf/hostlist</span><br><span class="line">[STEP 1 of 5] create <span class="built_in">local</span> ID and authorize on <span class="built_in">local</span> host</span><br><span class="line"></span><br><span class="line">[STEP 2 of 5] keyscan all hosts and update known_hosts file</span><br><span class="line"></span><br><span class="line">[STEP 3 of 5] authorize current user on remote hosts</span><br><span class="line">  ... send to mdw</span><br><span class="line">  ... send to sdw1</span><br><span class="line">  ***</span><br><span class="line">  *** Enter password <span class="keyword">for</span> sdw1: </span><br><span class="line">  ... send to sdw2</span><br><span class="line">  ... send to sdw3</span><br><span class="line"></span><br><span class="line">[STEP 4 of 5] determine common authentication file content</span><br><span class="line"></span><br><span class="line">[STEP 5 of 5] copy authentication files to all remote hosts</span><br><span class="line">  ... finished key exchange with mdw</span><br><span class="line">  ... finished key exchange with sdw1</span><br><span class="line">  ... finished key exchange with sdw2</span><br><span class="line">  ... finished key exchange with sdw3</span><br><span class="line"></span><br><span class="line">[INFO** completed successfully</span><br></pre></td></tr></table></figure>

<h4 id="交换成功后，后续就可以使用一些命令执行批量操作"><a href="#交换成功后，后续就可以使用一些命令执行批量操作" class="headerlink" title="交换成功后，后续就可以使用一些命令执行批量操作"></a>交换成功后，后续就可以使用一些命令执行批量操作</h4><p><strong>注意</strong>：使用<code>gpssh-exkeys</code>命令时一定要使用<code>gpadmin</code>用户，因为会在<code>/home/gpadmin/.ssh</code>中生成ssh的免密码登录秘钥，如果使用其它账号登录，则会在其它账号下生成密钥，在<code>gpadmin</code><br>账号下就无法使用<code>gpssh</code>的批处理命令</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[gpadmin@mdw ~]$ gpssh -f /home/gpadmin/conf/hostlist</span><br><span class="line">=&gt; pwd</span><br><span class="line">[sdw1] /home/gpadmin</span><br><span class="line">[sdw3] /home/gpadmin</span><br><span class="line">[ mdw] /home/gpadmin</span><br><span class="line">[sdw2] /home/gpadmin</span><br><span class="line">=&gt; ls</span><br><span class="line">[sdw1]</span><br><span class="line">[sdw3]</span><br><span class="line">[ mdw] conf  greenplum-db  greenplum-db-5.10.2</span><br><span class="line">[sdw2]</span><br><span class="line">=&gt; exit</span><br></pre></td></tr></table></figure>

<p>pwd命令是linux中的查看路径命令，在这里也是查看批量操作时各个节点当前所在的路径，从中可以看到已经成功连通了4个节点</p>
<h3 id="五、分发安装包到每个子节点"><a href="#五、分发安装包到每个子节点" class="headerlink" title="五、分发安装包到每个子节点"></a>五、分发安装包到每个子节点</h3><p>打包master节点上的安装包</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[gpadmin@mdw ~]$ tar -czf gp.tar.gz greenplum-db-5.10.2</span><br></pre></td></tr></table></figure>

<p>使用gpscp命令将这个文件复制到每个子节点</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[gpadmin@mdw ~]$ gpscp -f /home/gpadmin/conf/seg_hosts gp.tar.gz =:/home/gpadmin</span><br></pre></td></tr></table></figure>

<p>批量解压，并创建软链接</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[gpadmin@mdw ~]$ gpssh -f /home/gpadmin/conf/seg_hosts</span><br><span class="line">=&gt; tar -zxf gp.tar.gz</span><br><span class="line">[sdw3]</span><br><span class="line">[sdw1]</span><br><span class="line">[sdw2]</span><br><span class="line">=&gt; ln -s greenplum-db-5.10.2 greenplum-db</span><br><span class="line">[sdw3]</span><br><span class="line">[sdw2]</span><br><span class="line">[sdw1]</span><br></pre></td></tr></table></figure>

<p>这样就完成了所有子节点数据库的安装</p>
<h3 id="六、初始化安装数据库"><a href="#六、初始化安装数据库" class="headerlink" title="六、初始化安装数据库"></a>六、初始化安装数据库</h3><ul>
<li>批量创建数据目录</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[gpadmin@mdw ~]$ gpssh -f /home/gpadmin/conf/hostlist</span><br><span class="line">=&gt; mkdir gpdata</span><br><span class="line">[sdw1]</span><br><span class="line">[sdw3]</span><br><span class="line">[ mdw]</span><br><span class="line">[sdw2]</span><br><span class="line">=&gt; cd gpdata</span><br><span class="line">[sdw1]</span><br><span class="line">[sdw3]</span><br><span class="line">[ mdw]</span><br><span class="line">[sdw2]</span><br><span class="line">=&gt; mkdir gpmaster gpdatap1 gpdatap2 gpdatam1 gpdatam2</span><br><span class="line">[sdw1]</span><br><span class="line">[sdw3]</span><br><span class="line">[ mdw]</span><br><span class="line">[sdw2]</span><br><span class="line">=&gt; exit</span><br></pre></td></tr></table></figure>

<ul>
<li>在master节点上修改<code>.bash_profile</code>配置环境变量，并发送给其他子节点，确保这些环境变量生效</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[gpadmin@mdw ~]$ vi .bash_profile </span><br><span class="line">source /opt/gpadmin/greenplum-db/greenplum_path.sh</span><br><span class="line">export MASTER_DATA_DIRECTORY=/home/gpadmin/gpdata/gpmaster/gpseg-1</span><br><span class="line">export PGPORT=2345</span><br><span class="line">export PGDATABASE=testDB</span><br><span class="line">[gpadmin@mdw ~]$ source .bash_profile</span><br><span class="line">[gpadmin@mdw ~]$ gpscp -f /home/gpadmin/conf/seg_hosts /home/gpadmin/.bash_profile</span><br><span class="line">[gpadmin@sdw1 ~]$ source .bash_profile</span><br><span class="line">[gpadmin@sdw2 ~]$ source .bash_profile</span><br><span class="line">[gpadmin@sdw3 ~]$ source .bash_profile</span><br></pre></td></tr></table></figure>

<ul>
<li>初始化配置文件</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">[gpadmin@mdw ~]$ vi /home/gpadmin/conf/gpinitsystem_config</span><br><span class="line">ARRAY_NAME=&quot;Greenplum&quot;</span><br><span class="line">MACHINE_LIST_FILE=/home/gpadmin/conf/seg_hosts</span><br><span class="line"></span><br><span class="line"># Segment 的名称前缀</span><br><span class="line">SEG_PREFIX=gpseg</span><br><span class="line"># Primary Segment 起始的端口号</span><br><span class="line">PORT_BASE=33000</span><br><span class="line"># 指定 Primary Segment 的数据目录</span><br><span class="line">declare -a DATA_DIRECTORY=(/home/gpadmin/gpdata/gpdatap1 /home/gpadmin/gpdata/gpdatap2)</span><br><span class="line"># Master 所在机器的 Hostname</span><br><span class="line">MASTER_HOSTNAME=mdw</span><br><span class="line"># 指定 Master 的数据目录</span><br><span class="line">MASTER_DIRECTORY=/home/gpadmin/gpdata/gpmaster</span><br><span class="line"># Master 的端口 </span><br><span class="line">MASTER_PORT=2345</span><br><span class="line"># 指定Bash的版本</span><br><span class="line">TRUSTED_SHELL=/usr/bin/ssh</span><br><span class="line"># Mirror Segment起始的端口号</span><br><span class="line">MIRROR_PORT_BASE=43000</span><br><span class="line"># Primary Segment 主备同步的起始端口号</span><br><span class="line">REPLICATION_PORT_BASE=34000</span><br><span class="line"># Mirror Segment 主备同步的起始端口号</span><br><span class="line">MIRROR_REPLICATION_PORT_BASE=44000</span><br><span class="line"># Mirror Segment 的数据目录</span><br><span class="line">declare -a MIRROR_DATA_DIRECTORY=(/home/gpadmin/gpdata/gpdatam1 /home/gpadmin/gpdata/gpdatam2)</span><br></pre></td></tr></table></figure>

<ul>
<li>初始化数据库</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[gpadmin@mdw ~]$ gpinitsystem -c /home/gpadmin/conf/gpinitsystem_config -s sdw3</span><br></pre></td></tr></table></figure>

<p>其中，<code>-s sdw3</code>是指配置master的standby节点，然后按照提示步骤就能完成安装了</p>
<p>如果gpinitsystem不成功，在master节点的<code>/home/gpadmin/gpAdminLogs</code>目录下gpinitsystem_*<br>.log文件中查看日志信息，找出原因进行修改，然后再重新执行gpinitsystem进行初始化安装。</p>
<p>本文地址： <a href="https://github.com/maxzhao-it/blog/post/47598/">https://github.com/maxzhao-it/blog/post/47598/</a> </p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://github.com/maxzhao-it/blog/post/40115/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/blog/uploads/images/avatar.jpg">
      <meta itemprop="name" content="赵联胜">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="maxzhao">
      <meta itemprop="description" content="小码农赵联胜的博客,从遇到Java到爱上Java，工作之余学习Java生态圈中的各种技术。">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | maxzhao">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/blog/post/40115/" class="post-title-link" itemprop="url">PostgreSQL基于Citus的分布式</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2022-07-28 17:41:37" itemprop="dateModified" datetime="2022-07-28T17:41:37+08:00">2022-07-28</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/blog/categories/DB/" itemprop="url" rel="index"><span itemprop="name">DB</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/blog/categories/DB/PostgreSql/" itemprop="url" rel="index"><span itemprop="name">PostgreSql</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/blog/categories/DB/PostgreSql/%E5%88%86%E5%B8%83%E5%BC%8F/" itemprop="url" rel="index"><span itemprop="name">分布式</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="Citus简介"><a href="#Citus简介" class="headerlink" title="Citus简介"></a><strong>Citus</strong>简介</h2><p>Citus以插件的方式扩展到postgresql中，独立于postgresql内核，所以能很快的跟上pg主版本的更新，部署也比较简单，是现在非常流行的分布式方案。Citus在苏宁有大规模应用，微软也提供citus的商业支持。下面是citus的架构：</p>
<p><img src="/blog/uploads/images/2018042723003371.png" alt="2018042723003371"></p>
<p><code>Citus</code><br>节点主要分为协调节点和工作节点，协调节点不存储真实数据，只存储数据分布的元信息，实际的数据被分成若干分片，打散到不同worker节点中，应用连接协调节点，协调节点进行sql解析，生成分布式执行计划，下发到worker节点执行，cn将结果汇总返回客户端。</p>
<p><code>Citus</code> 的主要架构特点如下：</p>
<p>①有两种表类型：<strong>参考表</strong>和<strong>分布表</strong>，参考表每个协调节点和worker节点都有一份完整的副本，分布表则会打散分布到不同worker中。</p>
<p>②可以进行读写分离，如上图cn1为写节点，可以通过再增加多个cn读节点增加集群读的能力，<strong>写cn和读cn之间使用流复制进行元数据同步</strong>。</p>
<p>③支持MX模式，可以将元数据也存在某些worker节点中，这样使得该worker节点能够直接提供写的能力，以此增加集群写的能力。</p>
<p>④底层worker节点可以通过流复制搭建副本，保证数据高可用。</p>
<p>⑤做join时最好的结果是能够将计算下推到worker节点，但是只有在参考表和其他表做join以及两个表的分布方式相同的情况下才能下推到worker计算，否则需要将数据拉到协调节点进行计算。</p>
<p>⑥整体架构类似mycat的中间件，因为没有全局事务管理，故不能保证数据的实时读一致性，但是性能上相比要好。数据写一致性使用2pc来保证。</p>
<blockquote>
<p>来自 <a target="_blank" rel="noopener" href="https://cloud.tencent.com/developer/article/1563023">PostgreSQL的几种分布式架构对比</a></p>
</blockquote>
<p>其它特性：</p>
<p>● PostgreSQL兼容</p>
<p>● 水平扩展</p>
<p>● 实时并发查</p>
<p>● 快速数据加载</p>
<p>● 实时增删改查</p>
<p>● 持分布式事务</p>
<p>● 支持常用DDL</p>
<h3 id="交互"><a href="#交互" class="headerlink" title="交互"></a>交互</h3><ol>
<li>客户端应用访问数据时只和CN节点交互。</li>
<li>CN收到SQL请求后，生成分布式执行计划，并将各个子任务下发到相应的Worker节点，之后收集Worker的结果，经过处理后返回最终结果给客户端。</li>
</ol>
<h3 id="Citus-性能参考（来自互联网）"><a href="#Citus-性能参考（来自互联网）" class="headerlink" title="Citus 性能参考（来自互联网）"></a><code>Citus</code> 性能参考（来自互联网）</h3><p>为了能够直观的了解citus分片表的性能优势，下面在1个CN和8个worker组成citus集群上， 对比普通表和分片表(96分片)的性能差异。</p>
<p><img src="/blog/uploads/images/7481339-9a00b7b8322ce0d2.png" alt="img"></p>
<blockquote>
<p><strong>参考表：</strong>分片表主要解决的是大表的水平扩容问题，对数据量不是特别大又经常需要和分片表Join的维表可以采用一种特殊的分片策略，只分1个片且每个Worker上部署1个副本，这样的表叫做“参考表”。</p>
</blockquote>
<h3 id="推荐的业务场景"><a href="#推荐的业务场景" class="headerlink" title="推荐的业务场景"></a>推荐的业务场景</h3><h3 id="1、实时数据分析"><a href="#1、实时数据分析" class="headerlink" title="1、实时数据分析"></a>1、实时数据分析</h3><p>itus不仅支持高速的批量数据加载(20w&#x2F;s)，还支持单条记录的实时增删改查。<br>查询数据时，CN对每一个涉及的分片开一个连接驱动所有相关worker同时工作。并且支持过滤，投影，聚合，join等常见算子的下推，尽可能减少CN的负载。所以，对于count()，sum()<br>这类简单的聚合计算，在128分片时citus可以轻松获得和PostgreSQL单并发相比50倍以上的性能提升。</p>
<h3 id="2、多租户"><a href="#2、多租户" class="headerlink" title="2、多租户"></a>2、多租户</h3><p>和很多分布式数据库类似，citus对分片表间join的支持存在一定的限制。而多租户场景下每个租户的数据按租户ID分片，业务的SQL也带租户ID。因此这些SQL都可以直接下推到特定的分片上，避免了跨库join和跨库事务。</p>
<blockquote>
<p><strong>多租户定义：</strong>多租户技术或称多重租赁技术，简称SaaS，是<strong>一种软件架构技术</strong>，是实现如何在<strong>多用户环境下（此处的多用户一般是面向企业用户）共用相同的系统或程序组件</strong>，并且可<strong>确保各用户间数据的隔离性</strong><br>。简单讲：在一台服务器上运行单个应用实例，它为多个租户（客户）提供服务。从定义中我们可以理解：多租户是一种架构，目的是为了让多用户环境下使用同一套程序，且保证用户间数据隔离。那么重点就很浅显易懂了，**<br>多租户的重点就是同一套程序下实现多用户数据的隔离**。</p>
</blockquote>
<p>本文地址： <a href="https://github.com/maxzhao-it/blog/post/40115/">https://github.com/maxzhao-it/blog/post/40115/</a> </p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://github.com/maxzhao-it/blog/post/34819/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/blog/uploads/images/avatar.jpg">
      <meta itemprop="name" content="赵联胜">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="maxzhao">
      <meta itemprop="description" content="小码农赵联胜的博客,从遇到Java到爱上Java，工作之余学习Java生态圈中的各种技术。">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | maxzhao">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/blog/post/34819/" class="post-title-link" itemprop="url">PostgreSQL基于Greenplum的分布式</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2022-07-04 14:50:22" itemprop="dateModified" datetime="2022-07-04T14:50:22+08:00">2022-07-04</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/blog/categories/DB/" itemprop="url" rel="index"><span itemprop="name">DB</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/blog/categories/DB/PostgreSql/" itemprop="url" rel="index"><span itemprop="name">PostgreSql</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/blog/categories/DB/PostgreSql/%E5%88%86%E5%B8%83%E5%BC%8F/" itemprop="url" rel="index"><span itemprop="name">分布式</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="Greenplum简介"><a href="#Greenplum简介" class="headerlink" title="Greenplum简介"></a><strong>Greenplum</strong>简介</h2><h3 id="官方："><a href="#官方：" class="headerlink" title="官方："></a><a target="_blank" rel="noopener" href="https://cn.greenplum.org/">官方</a>：</h3><p><strong>全球首个</strong>开源、多云、并行 大数据平台</p>
<h3 id="来自网页"><a href="#来自网页" class="headerlink" title="来自网页"></a>来自网页</h3><p>Greenplum是pivotal公司推出的一款开源olap的mpp数据库，greenplum的用户在某种程度上甚至超越了pg，很多人可能是通过greenplum才认识的pg，可见greenplum的风靡。下面是greenplum架构：</p>
<p><img src="/blog/uploads/images/2018042723003372.png" alt="2018042723003371"></p>
<p>Master节点存储全局系统元数据信息，不存储真实数据。数据通过hash分布到不同的segment中，master作为sql的全局入口，负责在segment中分配工作负载，整合处理结果，返回客户端。</p>
<p>Greenplum架构特点如下：</p>
<p>①master节点可以做主备，segment节点也有镜像保证高可用，segment主备尽量混布到不同服务器上。</p>
<p>②支持行列混合存储引擎，同时支持外部表。</p>
<p>③在join时也涉及到数据跨节点重分布的问题，这也是<code>share nothing</code>数据库不可避免的问题。</p>
<p>④高速内部interconnect网络，实现数据join时的高速移动和汇总。</p>
<p>⑤高效的数据并行加载。</p>
<blockquote>
<p>来自 <a target="_blank" rel="noopener" href="https://cloud.tencent.com/developer/article/1563023">PostgreSQL的几种分布式架构对比</a></p>
</blockquote>
<h3 id="了解-share-nothing"><a href="#了解-share-nothing" class="headerlink" title="了解 share nothing"></a>了解 <code>share nothing</code></h3><ul>
<li><p><code>Shared Everthting</code>:一般是针对单个主机，完全透明共享CPU&#x2F;MEMORY&#x2F;IO，并行处理能力差，典型的代表SQLServer。 shared-everything架构优点很明显，但是网络，硬盘很容易就会成为系统瓶颈。</p>
</li>
<li><p><code>Shared Disk</code>：各个处理单元使用自己的私有 CPU和Memory，共享磁盘系统。典型的代表Oracle Rac，<br>它是数据共享，可通过增加节点来提高并行处理的能力，扩展能力较好。其类似于SMP（对称多处理）模式，但是当存储器接口达到饱和的时候，增加节点并不能获得更高的性能 。</p>
</li>
<li><p><code>Shared Nothing</code><br>：各个处理单元都有自己私有的CPU&#x2F;内存&#x2F;硬盘等，不存在共享资源，各处理单元之间通过协议通信，并行处理和扩展能力更好。各节点相互独立，各自处理自己的数据，处理后的结果可能向上层汇总或在节点间流转。Share-Nothing架构在扩展性和成本上都具有明显优势。</p>
</li>
</ul>
<h3 id="了解-MPP"><a href="#了解-MPP" class="headerlink" title="了解 MPP"></a>了解 MPP</h3><p>大规模并行处理系统是由许多松耦合处理单元组成的，借助MPP这种高性能的系统架构，Greenplum可以将TB级的数据仓库负载分解，并使用所有的系统资源并行处理单个查询。</p>
<p>本文地址： <a href="https://github.com/maxzhao-it/blog/post/34819/">https://github.com/maxzhao-it/blog/post/34819/</a> </p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://github.com/maxzhao-it/blog/post/50825/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/blog/uploads/images/avatar.jpg">
      <meta itemprop="name" content="赵联胜">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="maxzhao">
      <meta itemprop="description" content="小码农赵联胜的博客,从遇到Java到爱上Java，工作之余学习Java生态圈中的各种技术。">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | maxzhao">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/blog/post/50825/" class="post-title-link" itemprop="url">SpringBoot+kafka初始配置与使用</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2022-07-04 14:50:22" itemprop="dateModified" datetime="2022-07-04T14:50:22+08:00">2022-07-04</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/blog/categories/SpringBoot/" itemprop="url" rel="index"><span itemprop="name">SpringBoot</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/blog/categories/SpringBoot/kafka/" itemprop="url" rel="index"><span itemprop="name">kafka</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>这里使用 <code>SpringBoot 2.4.2</code> + <code>kafka 2.7.0</code></p>
<h2 id="开发配置测试"><a href="#开发配置测试" class="headerlink" title="开发配置测试"></a>开发配置测试</h2><h3 id="依赖"><a href="#依赖" class="headerlink" title="依赖"></a>依赖</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- Springboot 建议使用 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.kafka<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-kafka<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- 其它使用 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.kafka<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>kafka-clients<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.6.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">scope</span>&gt;</span>compile<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.kafka<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>kafka-streams<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.6.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">scope</span>&gt;</span>compile<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">optional</span>&gt;</span>true<span class="tag">&lt;/<span class="name">optional</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h3 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h3><p>Kafka在属性文件格式中使用键值对进行配置。这些值可以通过文件或编程方式提供。 必备配置如下:</p>
<ol>
<li>broker.id&#x3D;0</li>
<li>log.dirs&#x3D;&#x2F;tmp&#x2F;kafka-logs</li>
<li>zookeeper.connect&#x3D;hostname1:port1,hostname2:port2,hostname3:port3</li>
<li>auto.create.topics.enable&#x3D;true 是否允许在服务器上自动创建topic</li>
</ol>
<p><code>application.yml</code> 配置</p>
<figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">spring</span></span><br><span class="line"><span class="attr">kafka:</span></span><br><span class="line">  <span class="comment">#    Kafka集群</span></span><br><span class="line">  <span class="attr">bootstrap-servers:</span> <span class="number">127.0</span><span class="number">.0</span><span class="number">.1</span><span class="string">:9092</span></span><br><span class="line">  <span class="attr">producer:</span></span><br><span class="line">    <span class="comment"># 重试次数</span></span><br><span class="line">    <span class="attr">retries:</span> <span class="number">0</span></span><br><span class="line">    <span class="comment"># 应答级别:多少个分区副本备份完成时向生产者发送ack确认(可选0、1、all/-1)</span></span><br><span class="line">    <span class="attr">acks:</span> <span class="number">1</span></span><br><span class="line">    <span class="comment"># 批量大小</span></span><br><span class="line">    <span class="attr">batch-size:</span> <span class="number">10240</span></span><br><span class="line">    <span class="comment"># 提交延时</span></span><br><span class="line">    <span class="attr">properties.linger.ms:</span> <span class="number">0</span></span><br><span class="line">    <span class="comment"># 当生产端积累的消息达到batch-size或接收到消息linger.ms后,生产者就会将消息提交给kafka</span></span><br><span class="line">    <span class="comment"># linger.ms为0表示每接收到一条消息就提交给kafka,这时候batch-size其实就没用了</span></span><br><span class="line">    <span class="comment"># 生产端缓冲区大小</span></span><br><span class="line">    <span class="attr">buffer-memory:</span> <span class="number">33554432</span></span><br><span class="line">    <span class="comment"># Kafka提供的序列化和反序列化类</span></span><br><span class="line">    <span class="attr">key-serializer:</span> <span class="string">org.apache.kafka.common.serialization.StringSerializer</span></span><br><span class="line">    <span class="attr">value-serializer:</span> <span class="string">org.apache.kafka.common.serialization.StringSerializer</span></span><br><span class="line">  <span class="comment"># 自定义分区器</span></span><br><span class="line">  <span class="comment"># spring.kafka.producer.properties.partitioner.class: com.felix.kafka.producer.CustomizePartitioner</span></span><br><span class="line">  <span class="comment"># 初始化消费者配置</span></span><br><span class="line">  <span class="comment"># 默认的消费组ID</span></span><br><span class="line">  <span class="attr">consumer:</span></span><br><span class="line">    <span class="attr">properties:</span></span><br><span class="line">      <span class="attr">group:</span></span><br><span class="line">        <span class="attr">id:</span> <span class="string">defaultConsumerGroup</span></span><br><span class="line">    <span class="comment"># 是否自动提交offset</span></span><br><span class="line">    <span class="attr">enable-auto-commit:</span> <span class="literal">true</span></span><br><span class="line">    <span class="comment"># 提交offset延时(接收到消息后多久提交offset)</span></span><br><span class="line">    <span class="attr">auto:</span></span><br><span class="line">      <span class="attr">commit:</span></span><br><span class="line">        <span class="attr">interval:</span></span><br><span class="line">          <span class="attr">ms:</span> <span class="number">1000</span></span><br><span class="line">    <span class="comment"># 当kafka中没有初始offset或offset超出范围时将自动重置offset</span></span><br><span class="line">    <span class="comment"># earliest:重置为分区中最小的offset;</span></span><br><span class="line">    <span class="comment"># latest:重置为分区中最新的offset(消费分区中新产生的数据);</span></span><br><span class="line">    <span class="comment"># none:只要有一个分区不存在已提交的offset,就抛出异常;</span></span><br><span class="line">    <span class="attr">auto-offset-reset:</span> <span class="string">latest</span></span><br><span class="line">    <span class="comment"># 消费会话超时时间(超过这个时间consumer没有发送心跳,就会触发rebalance操作)</span></span><br><span class="line">    <span class="attr">properties.session.timeout.ms:</span> <span class="number">120000</span></span><br><span class="line">    <span class="comment"># 消费请求超时时间</span></span><br><span class="line">    <span class="attr">properties.request.timeout.ms:</span> <span class="number">180000</span></span><br><span class="line">    <span class="comment"># Kafka提供的序列化和反序列化类</span></span><br><span class="line">    <span class="attr">key-deserializer:</span> <span class="string">org.apache.kafka.common.serialization.StringDeserializer</span></span><br><span class="line">    <span class="attr">value-deserializer:</span> <span class="string">org.apache.kafka.common.serialization.StringDeserializer</span></span><br><span class="line">    <span class="comment"># 批量消费每次最多消费多少条消息</span></span><br><span class="line">  <span class="comment">#      max-poll-records: 50</span></span><br><span class="line">  <span class="attr">listener:</span></span><br><span class="line">    <span class="comment"># 消费端监听的topic不存在时，项目启动会报错(关掉)</span></span><br><span class="line">    <span class="attr">missing-topics-fatal:</span> <span class="literal">false</span></span><br><span class="line">    <span class="comment"># 设置批量消费</span></span><br><span class="line"><span class="comment">#      type: batch</span></span><br></pre></td></tr></table></figure>

<h3 id="创建-topic-的类"><a href="#创建-topic-的类" class="headerlink" title="创建 topic 的类"></a>创建 topic 的类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 初始化默认 kafka 分区</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> maxzhao</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@date</span> 2021-01-26 14:37</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Configuration</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">DefaultTestKafkaInitialConfiguration</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">String</span> <span class="variable">TEST_TOPIC</span> <span class="operator">=</span> <span class="string">&quot;test-topic&quot;</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 创建一个 test-topic 的 Topic，设置分区数为 1,分区副本数为 1</span></span><br><span class="line"><span class="comment">     * &lt;p&gt;修改分区只需要重新配置分区数、分区副本数，然后重启（数量只能增大）&lt;/p&gt;</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Bean</span></span><br><span class="line">    <span class="keyword">public</span> NewTopic <span class="title function_">initialTopic</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">NewTopic</span>(TEST_TOPIC, <span class="number">1</span>, (<span class="type">short</span>) <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="简单的生产者"><a href="#简单的生产者" class="headerlink" title="简单的生产者"></a>简单的生产者</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">interface</span> <span class="title class_">IKafkaService</span> &#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 发送消息</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> topic   主题</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> message 消息</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">void</span> <span class="title function_">sendMessage</span><span class="params">(String topic, String message)</span>;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Slf4j</span></span><br><span class="line"><span class="meta">@Service</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">KafkaServiceImpl</span> <span class="keyword">implements</span> <span class="title class_">IKafkaService</span> &#123;</span><br><span class="line">    <span class="meta">@Autowired</span></span><br><span class="line">    <span class="keyword">private</span> KafkaTemplate&lt;String, Object&gt; kafkaTemplate;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">sendMessage</span><span class="params">(String topic, String message)</span> &#123;</span><br><span class="line">        kafkaTemplate.send(topic, message);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="简单的消费者"><a href="#简单的消费者" class="headerlink" title="简单的消费者"></a>简单的消费者</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="meta">@Slf4j</span></span><br><span class="line"><span class="meta">@Component</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">KafkaConsumer</span> &#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 消费监听</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@KafkaListener(topics = &#123;DefaultTestKafkaInitialConfiguration.TEST_TOPIC&#125;)</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onMessage1</span><span class="params">(ConsumerRecord&lt;?, ?&gt; record)</span> &#123;</span><br><span class="line">        log.debug(<span class="string">&quot;消费：&#123;&#125;-&#123;&#125;-&#123;&#125;&quot;</span>, record.topic(), record.partition(), record.value());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="简单测试"><a href="#简单测试" class="headerlink" title="简单测试"></a>简单测试</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="meta">@Slf4j</span></span><br><span class="line"><span class="meta">@RunWith(SpringRunner.class)</span></span><br><span class="line"><span class="meta">@SpringBootTest(classes = start.DemoApplication.class)</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">KafkaTest</span> &#123;</span><br><span class="line">    <span class="meta">@Autowired</span></span><br><span class="line">    <span class="keyword">private</span> IKafkaService kafkaService;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 测试消息发送</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">sendMessage</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="type">String</span> <span class="variable">message</span> <span class="operator">=</span> <span class="string">&quot;这是一个简单的消息！&quot;</span>;</span><br><span class="line">        log.debug(<span class="string">&quot;发送消息：&#123;&#125;-&#123;&#125;&quot;</span>, DefaultTestKafkaInitialConfiguration.TEST_TOPIC, message);</span><br><span class="line">        kafkaService.sendMessage(DefaultTestKafkaInitialConfiguration.TEST_TOPIC, message);</span><br><span class="line">        log.debug(<span class="string">&quot;发送完成&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>输出结果</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">2021-01-26 15:40:33 DEBUG (KafkaTest.java:31)- 发送消息：test-topic-这是一个简单的消息！</span><br><span class="line">2021-01-26 15:49:32 DEBUG (KafkaTest.java:33)- 发送完成</span><br><span class="line">2021-01-26 15:49:48 DEBUG (KafkaConsumer.java:24)- 消费：test-topic-0-这是一个简单的消息！</span><br></pre></td></tr></table></figure>

<h4 id="带回调的生产者"><a href="#带回调的生产者" class="headerlink" title="带回调的生产者"></a>带回调的生产者</h4><p><code>kafkaTemplate</code> 提供了一个回调方法<code>addCallback</code>，可以在回调方法中监控消息是否发送成功，或失败时做补偿处理 有集中写法，这里简单的介绍两种 <code>lambda</code>和接口</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="meta">@Slf4j</span></span><br><span class="line"><span class="meta">@Service(&quot;kafkaService&quot;)</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">KafkaServiceImpl</span> <span class="keyword">implements</span> <span class="title class_">IKafkaService</span> &#123;</span><br><span class="line">    <span class="meta">@Autowired</span></span><br><span class="line">    <span class="keyword">private</span> KafkaTemplate&lt;String, Object&gt; kafkaTemplate;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">sendMessage</span><span class="params">(String topic, String message)</span> &#123;</span><br><span class="line">        kafkaTemplate.send(topic, message).addCallback(success -&gt; &#123;</span><br><span class="line">            <span class="keyword">if</span> (success == <span class="literal">null</span>) &#123;</span><br><span class="line">                log.debug(<span class="string">&quot;消息发送成功，响应数据不存在&quot;</span>);</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="type">RecordMetadata</span> <span class="variable">recordMetadata</span> <span class="operator">=</span> success.getRecordMetadata();</span><br><span class="line">                <span class="comment">/*消息发送到的topic,消息发送到的分区 partition,消息在分区内的offset*/</span></span><br><span class="line">                log.debug(<span class="string">&quot;消息发送成功：topic:&#123;&#125; partition:&#123;&#125; offset:&#123;&#125;&quot;</span>, recordMetadata.topic(), recordMetadata.partition(), recordMetadata.offset());</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;, throwable -&gt; &#123;</span><br><span class="line">            log.warn(<span class="string">&quot;消息发送失败：topic:&#123;&#125; message&#123;&#125; error:&#123;&#125;&quot;</span>, topic, message, throwable.getMessage());</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        kafkaTemplate.send(topic, message).addCallback(<span class="keyword">new</span> <span class="title class_">ListenableFutureCallback</span>&lt;SendResult&lt;String, Object&gt;&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onSuccess</span><span class="params">(SendResult&lt;String, Object&gt; success)</span> &#123;</span><br><span class="line">                <span class="keyword">if</span> (success == <span class="literal">null</span>) &#123;</span><br><span class="line">                    log.debug(<span class="string">&quot;消息发送成功，响应数据不存在&quot;</span>);</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    <span class="type">RecordMetadata</span> <span class="variable">recordMetadata</span> <span class="operator">=</span> success.getRecordMetadata();</span><br><span class="line">                    <span class="comment">/*消息发送到的topic,消息发送到的分区 partition,消息在分区内的offset*/</span></span><br><span class="line">                    log.debug(<span class="string">&quot;消息发送成功：topic:&#123;&#125; partition:&#123;&#125; offset:&#123;&#125;&quot;</span>, recordMetadata.topic(), recordMetadata.partition(), recordMetadata.offset());</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onFailure</span><span class="params">(Throwable throwable)</span> &#123;</span><br><span class="line">                log.warn(<span class="string">&quot;消息发送失败：topic:&#123;&#125; message&#123;&#125; error:&#123;&#125;&quot;</span>, topic, message, throwable.getMessage());</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="自定义分区器"><a href="#自定义分区器" class="headerlink" title="自定义分区器"></a>自定义分区器</h3><p>我们知道，kafka中每个topic被划分为多个分区，那么生产者将消息发送到topic时，具体追加到哪个分区呢？这就是所谓的分区策略，Kafka 为我们提供了默认的分区策略，同时它也支持自定义分区策略。其路由机制为：</p>
<ol>
<li>若发送消息时指定了分区（即自定义分区策略），则直接将消息append到指定分区；</li>
<li>若发送消息时未指定 <code>partition</code>，但指定了 key（kafka允许为每条消息设置一个key），则对key值进行hash计算，根据计算结果路由到指定分区，这种情况下可以保证同一个 Key 的所有消息都进入到相同的分区；</li>
<li><code>partition</code> 和 key 都未指定，则使用kafka默认的分区策略，轮询选出一个 <code>partition</code>；</li>
</ol>
<p>※ 我们来自定义一个分区策略，将消息发送到我们指定的 <code>partition</code>，首先新建一个分区器类实现 <code>Partitioner</code> 接口，重写方法，其中partition方法的返回值就表示将消息发送到几号分区，</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">CustomPartitioner</span> <span class="keyword">implements</span> <span class="title class_">Partitioner</span> &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">partition</span><span class="params">(String topic, Object key, <span class="type">byte</span>[] keyBytes, Object value, <span class="type">byte</span>[] valueBytes, Cluster cluster)</span> &#123;</span><br><span class="line">        <span class="comment">// 自定义分区规则</span></span><br><span class="line">        <span class="comment">// ......</span></span><br><span class="line">        <span class="comment">// 0 则是默认发到 0号</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">close</span><span class="params">()</span> &#123;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">configure</span><span class="params">(Map&lt;String, ?&gt; configs)</span> &#123;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>在<code>application.yml</code>中配置自定义分区器，配置的值就是分区器类的全路径名，</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 自定义分区器</span><br><span class="line">spring.kafka.producer.properties.partitioner.class=gt.maxzhao.mq.config.CustomPartitioner</span><br></pre></td></tr></table></figure>

<h3 id="kafka事务提交"><a href="#kafka事务提交" class="headerlink" title="kafka事务提交"></a><code>kafka</code>事务提交</h3><p>如果在发送消息时需要创建事务，可以使用 KafkaTemplate 的 executeInTransaction 方法来声明事务，</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="meta">@Slf4j</span></span><br><span class="line"><span class="meta">@Service(&quot;kafkaService&quot;)</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">KafkaServiceImpl</span> <span class="keyword">implements</span> <span class="title class_">IKafkaService</span> &#123;</span><br><span class="line">    <span class="meta">@Autowired</span></span><br><span class="line">    <span class="keyword">private</span> KafkaTemplate&lt;String, Object&gt; kafkaTemplate;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">sendMessageWithTransaction</span><span class="params">(String topic, String message)</span> &#123;</span><br><span class="line">        kafkaTemplate.executeInTransaction(operations -&gt; &#123;</span><br><span class="line">            operations.send(topic, message).addCallback(success -&gt; &#123;</span><br><span class="line">                <span class="comment">/*没有发出去，就没有响应*/</span></span><br><span class="line">            &#125;, throwable -&gt; &#123;</span><br><span class="line">                <span class="comment">/*没有发出去，就没有响应*/</span></span><br><span class="line">            &#125;);</span><br><span class="line">            <span class="comment">/*这里抛出错误，则不会发出消息*/</span></span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">RuntimeException</span>();</span><br><span class="line">        &#125;);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="消费者测试"><a href="#消费者测试" class="headerlink" title="消费者测试"></a>消费者测试</h3><h4 id="订阅"><a href="#订阅" class="headerlink" title="订阅"></a>订阅</h4><p>订阅 <code>topic</code> 是以组的形式进行的</p>
<ul>
<li>同一组的同一个<code>partition</code> 只能被消费一次。</li>
<li>不同组可以共同消费同一个 <code>partition</code>。</li>
<li>同一组的监听数量大于 <code>partition</code>，那么一定有监听空闲。</li>
</ul>
<h4 id="简单消费"><a href="#简单消费" class="headerlink" title="简单消费"></a>简单消费</h4><p>1、指定topic、partition、offset消费</p>
<p>前面监听消费 topic 的时候，监听的是 topic 上所有的消息，如果想指定指定partition、指定offset来消费，直接配置<code>@KafkaListener</code>注解。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="meta">@Slf4j</span></span><br><span class="line"><span class="meta">@Component</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">KafkaConsumer</span> &#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 消费监听</span></span><br><span class="line"><span class="comment">     * &lt;p&gt;可以监听多个&lt;/p&gt;</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@KafkaListener(topics = &#123;DefaultTestKafkaInitialConfiguration.TEST_TOPIC&#125;)</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onMessage1</span><span class="params">(ConsumerRecord&lt;?, ?&gt; record)</span> &#123;</span><br><span class="line">        log.debug(<span class="string">&quot;消费：&#123;&#125;-&#123;&#125;-&#123;&#125;&quot;</span>, record.topic(), record.partition(), record.value());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     *  指定topic、partition、offset消费</span></span><br><span class="line"><span class="comment">     * &lt;p&gt;同时监听topic1和topic2，监听topic1的0号分区、topic2的 &quot;0号和1号&quot; 分区，指向1号分区的offset初始值为8&lt;/p&gt;</span></span><br><span class="line"><span class="comment">     **/</span></span><br><span class="line">    <span class="meta">@KafkaListener(id = &quot;consumer1&quot;, groupId = &quot;consumer-group&quot;,</span></span><br><span class="line"><span class="meta">            topicPartitions = &#123;</span></span><br><span class="line"><span class="meta">                    @TopicPartition(topic = DefaultTestKafkaInitialConfiguration.TEST_TOPIC, partitions = &#123;&quot;0&quot;&#125;),</span></span><br><span class="line"><span class="meta">                    @TopicPartition(topic = DefaultTestKafkaInitialConfiguration.TEST_TOPIC_2, partitions = &quot;0&quot;,</span></span><br><span class="line"><span class="meta">                            partitionOffsets = @PartitionOffset(partition = &quot;1&quot;, initialOffset = &quot;8&quot;))</span></span><br><span class="line"><span class="meta">            &#125;)</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onMessage2</span><span class="params">(ConsumerRecord&lt;?, ?&gt; record)</span> &#123;</span><br><span class="line">        log.debug(<span class="string">&quot;消费2：topic:&#123;&#125; partition:&#123;&#125; value:&#123;&#125; offset:&#123;&#125;&quot;</span>, record.topic(), record.partition(), record.value(), record.offset());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><code>onMessage2</code>监听的含义：监听 <code>TEST_TOPIC</code> 的0号分区，同时监听 <code>TEST_TOPIC_2</code> 的0号分区和 <code>TEST_TOPIC_2</code> 的1号分区里面 <code>offset</code> 从8开始的消息。</p>
<p>属性解释：</p>
<ul>
<li><code>id</code>：消费者ID；</li>
<li><code>groupId</code>：消费组ID；</li>
<li><code>topics</code>：监听的 <code>topic</code>，可监听多个；</li>
<li><code>topicPartitions</code>：可配置更加详细的监听信息，可指定 <code>topic</code>、<code>partition</code>、<code>offset</code> 监听。</li>
<li>注意：<code>topics</code> 和 <code>topicPartitions</code> 不能同时使用；</li>
</ul>
<p>最终结果会发现，<code>onMessage1</code>、<code>onMessage2</code>都可以消费 <code>TEST_TOPIC</code> 的消息。</p>
<h4 id="批量消费"><a href="#批量消费" class="headerlink" title="批量消费"></a>批量消费</h4><p>设置 <code>application.yml</code> 开启批量消费即可，</p>
<figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置批量消费</span></span><br><span class="line"><span class="attr">spring.kafka.listener.type:</span> <span class="string">batch</span></span><br><span class="line"><span class="comment"># 批量消费每次最多消费50条消息</span></span><br><span class="line"><span class="attr">spring.kafka.consumer.max-poll-records:</span> <span class="number">50</span></span><br><span class="line"><span class="comment"># 调整延时时间，否则无法一次接收多个信息</span></span><br><span class="line"><span class="attr">spring.kafka.producer.properties.linger.ms:</span> <span class="number">50</span></span><br></pre></td></tr></table></figure>

<p>接收消息时用List来接收，监听代码如下:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="meta">@Slf4j</span></span><br><span class="line"><span class="meta">@Component</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">KafkaConsumer</span> &#123;</span><br><span class="line">    <span class="comment">/******************************************************************/</span></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 批量消费</span></span><br><span class="line"><span class="comment">     **/</span></span><br><span class="line">    <span class="meta">@KafkaListener(id = &quot;consumer2&quot;, groupId = &quot;consumer-group&quot;,</span></span><br><span class="line"><span class="meta">            topics = &#123;DefaultTestKafkaInitialConfiguration.TEST_TOPIC_3&#125;)</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onMessage3</span><span class="params">(List&lt;ConsumerRecord&lt;?, ?&gt;&gt; records)</span> &#123;</span><br><span class="line">        <span class="keyword">for</span> (ConsumerRecord&lt;?, ?&gt; record : records) &#123;</span><br><span class="line">            log.debug(<span class="string">&quot;批量消费：topic:&#123;&#125; partition:&#123;&#125; value:&#123;&#125; offset:&#123;&#125;&quot;</span>, record.topic(), record.partition(), record.value(), record.offset());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Slf4j</span></span><br><span class="line"><span class="meta">@RunWith(SpringRunner.class)</span></span><br><span class="line"><span class="meta">@SpringBootTest(classes = start.DemoApplication.class)</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">KafkaTest</span> &#123;</span><br><span class="line">    <span class="meta">@Autowired</span></span><br><span class="line">    <span class="keyword">private</span> IKafkaService kafkaService;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 测试发送多个消息发送</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">sendMessage3</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="type">String</span> <span class="variable">message</span> <span class="operator">=</span> <span class="string">&quot;这是一个简单的消息！&quot;</span>;</span><br><span class="line">        <span class="type">String</span> <span class="variable">message2</span> <span class="operator">=</span> <span class="string">&quot;这是一个简单的消息！_2&quot;</span>;</span><br><span class="line">        <span class="type">String</span> <span class="variable">message3</span> <span class="operator">=</span> <span class="string">&quot;这是一个简单的消息！_3&quot;</span>;</span><br><span class="line">        <span class="type">String</span> <span class="variable">message4</span> <span class="operator">=</span> <span class="string">&quot;这是一个简单的消息！_4&quot;</span>;</span><br><span class="line">        log.debug(<span class="string">&quot;发送消息：&#123;&#125;-&#123;&#125;&quot;</span>, DefaultTestKafkaInitialConfiguration.TEST_TOPIC_3, message);</span><br><span class="line">        kafkaService.sendMessage(DefaultTestKafkaInitialConfiguration.TEST_TOPIC_3, message);</span><br><span class="line">        kafkaService.sendMessage(DefaultTestKafkaInitialConfiguration.TEST_TOPIC_3, message2);</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            Thread.sleep(<span class="number">2000</span>);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">        &#125;</span><br><span class="line">        kafkaService.sendMessage(DefaultTestKafkaInitialConfiguration.TEST_TOPIC_3, message3);</span><br><span class="line">        kafkaService.sendMessage(DefaultTestKafkaInitialConfiguration.TEST_TOPIC_3, message4);</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            Thread.sleep(<span class="number">5000</span>);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">        &#125;</span><br><span class="line">        log.debug(<span class="string">&quot;发送完成&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>输出结果为</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">2021-01-26 20:23:20 DEBUG (KafkaConsumer.java:52)- 批量消费：topic:test-topic_3 partition:0 value:这是一个简单的消息！ offset:10</span><br><span class="line">2021-01-26 20:23:20 DEBUG (KafkaConsumer.java:52)- 批量消费：topic:test-topic_3 partition:0 value:这是一个简单的消息！_2 offset:11</span><br><span class="line">2021-01-26 20:23:22 DEBUG (KafkaConsumer.java:52)- 批量消费：topic:test-topic_3 partition:0 value:这是一个简单的消息！_3 offset:12</span><br><span class="line">2021-01-26 20:23:22 DEBUG (KafkaConsumer.java:52)- 批量消费：topic:test-topic_3 partition:0 value:这是一个简单的消息！_4 offset:13</span><br></pre></td></tr></table></figure>

<h4 id="ConsumerAwareListenerErrorHandler-异常处理器"><a href="#ConsumerAwareListenerErrorHandler-异常处理器" class="headerlink" title="ConsumerAwareListenerErrorHandler 异常处理器"></a><code>ConsumerAwareListenerErrorHandler</code> 异常处理器</h4><p>通过异常处理器，我们可以处理consumer在消费时发生的异常。 新建一个 <code>ConsumerAwareListenerErrorHandler</code> 类型的异常处理方法，用 <code>@Bean</code> 注入， 然后我们将这个异常处理器的 <code>Bean</code><br>放到 <code>@KafkaListener</code> 注解的 <code>errorHandler</code> 属性里面，当监听抛出异常的时候，则会自动调用异常处理器，</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="meta">@Slf4j</span></span><br><span class="line"><span class="meta">@Component</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">KafkaConsumer</span> &#123;</span><br><span class="line">    <span class="comment">/********************************************************/</span></span><br><span class="line">    <span class="meta">@Bean</span></span><br><span class="line">    <span class="keyword">public</span> ConsumerAwareListenerErrorHandler <span class="title function_">consumerAwareErrorHandler</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> (message, exception, consumer) -&gt; &#123;</span><br><span class="line">            log.debug(<span class="string">&quot;批量消费异常：topic:&#123;&#125; message:&#123;&#125;&quot;</span>,</span><br><span class="line">                    consumer.listTopics(),</span><br><span class="line">                    message.getPayload());</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">null</span>;</span><br><span class="line">        &#125;;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 批量消费</span></span><br><span class="line"><span class="comment">     **/</span></span><br><span class="line">    <span class="meta">@KafkaListener(id = &quot;consumer4&quot;, groupId = &quot;consumer-group&quot;,</span></span><br><span class="line"><span class="meta">            topics = &#123;DefaultTestKafkaInitialConfiguration.TEST_TOPIC_4&#125;,</span></span><br><span class="line"><span class="meta">            errorHandler = &quot;consumerAwareErrorHandler&quot;)</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onMessage4</span><span class="params">(List&lt;ConsumerRecord&lt;?, ?&gt;&gt; records)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">Exception</span>(<span class="string">&quot;批量消费-模拟异常&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Slf4j</span></span><br><span class="line"><span class="meta">@RunWith(SpringRunner.class)</span></span><br><span class="line"><span class="meta">@SpringBootTest(classes = start.DemoApplication.class)</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">KafkaTest</span> &#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 测试异常处理</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">sendMessage4</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="type">String</span> <span class="variable">message</span> <span class="operator">=</span> <span class="string">&quot;这是一个简单的消息_4！&quot;</span>;</span><br><span class="line">        log.debug(<span class="string">&quot;发送消息：&#123;&#125;-&#123;&#125;&quot;</span>, DefaultTestKafkaInitialConfiguration.TEST_TOPIC_4, message);</span><br><span class="line">        kafkaService.sendMessage(DefaultTestKafkaInitialConfiguration.TEST_TOPIC_4, message);</span><br><span class="line">        log.debug(<span class="string">&quot;发送完成&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>执行看一下效果，</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2021-01-26 20:36:34 DEBUG (KafkaConsumer.java:61)- 批量消费异常：topic:&#123;test-topic_2=[Partition(topic = test-topic_2, partition = 0, leader = 0, replicas = [0], isr = [0], offlineReplicas = [])], test=[Partition(topic =******************************serialized key size = -1, serialized value size = 32, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = 这是一个简单的消息_4！)]</span><br></pre></td></tr></table></figure>

<h4 id="消息过滤器"><a href="#消息过滤器" class="headerlink" title="消息过滤器"></a>消息过滤器</h4><p>消息过滤器可以在消息抵达consumer之前被拦截，在实际应用中，我们可以根据自己的业务逻辑，筛选出需要的信息再交由 <code>KafkaListener</code> 处理，不需要的消息则过滤掉。 配置消息过滤只需要为 监听器工厂<br>配置一个 <code>RecordFilterStrategy</code>（消息过滤策略），返回 <code>true</code> 的时候消息将会被抛弃，返回 <code>false</code> 时，消息能正常抵达监听容器。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="meta">@Slf4j</span></span><br><span class="line"><span class="meta">@Component</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">KafkaConsumer</span> &#123;</span><br><span class="line">    <span class="comment">/********************************************************/</span></span><br><span class="line"></span><br><span class="line">    <span class="meta">@Autowired</span></span><br><span class="line">    <span class="keyword">private</span> ConsumerFactory consumerFactory;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 消息过滤器</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Bean</span></span><br><span class="line">    <span class="keyword">public</span> ConcurrentKafkaListenerContainerFactory <span class="title function_">filterContainerFactory</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="type">ConcurrentKafkaListenerContainerFactory</span> <span class="variable">factory</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">ConcurrentKafkaListenerContainerFactory</span>();</span><br><span class="line">        factory.setConsumerFactory(consumerFactory);</span><br><span class="line">        <span class="comment">// 被过滤的消息将被丢弃</span></span><br><span class="line">        factory.setAckDiscarded(<span class="literal">true</span>);</span><br><span class="line">        <span class="comment">// 消息过滤策略</span></span><br><span class="line">        factory.setRecordFilterStrategy(consumerRecord -&gt; &#123;</span><br><span class="line">            <span class="keyword">if</span> (consumerRecord.value().toString().length() &gt; <span class="number">20</span>) &#123;</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">//返回true消息则被过滤</span></span><br><span class="line">            <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">        &#125;);</span><br><span class="line">        <span class="keyword">return</span> factory;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 消息过滤</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@KafkaListener(id = &quot;consumer5&quot;, groupId = &quot;consumer-group&quot;,</span></span><br><span class="line"><span class="meta">            topics = &#123;DefaultTestKafkaInitialConfiguration.TEST_TOPIC_5&#125;,</span></span><br><span class="line"><span class="meta">            containerFactory = &quot;filterContainerFactory&quot;)</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onMessage5</span><span class="params">(String record)</span> &#123;</span><br><span class="line">        log.debug(<span class="string">&quot;批量消费-拦截长度小于20的字符串： value:&#123;&#125; &quot;</span>, record);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Slf4j</span></span><br><span class="line"><span class="meta">@RunWith(SpringRunner.class)</span></span><br><span class="line"><span class="meta">@SpringBootTest(classes = start.DemoApplication.class)</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">KafkaTest</span> &#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 消息过滤</span></span><br><span class="line"><span class="comment">     * &lt;p&gt;需要吧提交延时改为0&lt;/p&gt;</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">sendMessage5</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="type">String</span> <span class="variable">message</span> <span class="operator">=</span> <span class="string">&quot;这是一个简单的消息_5！&quot;</span>;</span><br><span class="line">        <span class="type">String</span> <span class="variable">message2</span> <span class="operator">=</span> <span class="string">&quot;这是一个简单的消息_5！__________________________________________________&quot;</span>;</span><br><span class="line">        kafkaService.sendMessage(DefaultTestKafkaInitialConfiguration.TEST_TOPIC_5, message);</span><br><span class="line">        kafkaService.sendMessage(DefaultTestKafkaInitialConfiguration.TEST_TOPIC_5, message2);</span><br><span class="line">        log.debug(<span class="string">&quot;发送完成&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>输出结果</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">2021-01-26 21:03:10 DEBUG (KafkaTest.java:96)- 发送完成</span><br><span class="line">2021-01-26 21:03:10 DEBUG (KafkaServiceImpl.java:33)- 消息发送成功：topic:test-topic_5 partition:0 offset:26</span><br><span class="line">2021-01-26 21:03:10 DEBUG (KafkaConsumer.java:112)- 批量消费-拦截长度小于20的字符串： value:这是一个简单的消息_5！__________________________________________________ </span><br><span class="line">2021-01-26 21:03:10 DEBUG (KafkaServiceImpl.java:33)- 消息发送成功：topic:test-topic_5 partition:0 offset:27</span><br></pre></td></tr></table></figure>

<h4 id="消息转发"><a href="#消息转发" class="headerlink" title="消息转发"></a>消息转发</h4><p>消息转发在实际开发中，应用A从<code>TopicA</code> 获取到消息，经过处理后转发到<code>TopicB</code>， 再由应用B监听处理消息，即一个应用处理完成后将该消息转发至其他应用，完成消息的转发。<br>在<code>SpringBoot</code>集成<code>Kafka</code>实现消息的转发，只需要通过一个<code>@SendTo</code> 注解，被注解方法的<code>return</code>值即转发的消息内容，如下，</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="meta">@Slf4j</span></span><br><span class="line"><span class="meta">@Component</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">KafkaConsumer</span> &#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 消息转发给 5</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@KafkaListener(id = &quot;consumer6&quot;, groupId = &quot;consumer-group&quot;,</span></span><br><span class="line"><span class="meta">            topics = &#123;DefaultTestKafkaInitialConfiguration.TEST_TOPIC_6&#125;)</span></span><br><span class="line">    <span class="meta">@SendTo(DefaultTestKafkaInitialConfiguration.TEST_TOPIC_5)</span></span><br><span class="line">    <span class="keyword">public</span> String <span class="title function_">onMessage6</span><span class="params">(List&lt;ConsumerRecord&lt;?, ?&gt;&gt; records)</span> &#123;</span><br><span class="line">        log.debug(<span class="string">&quot;消息转发 6：topic:&#123;&#125; partition:&#123;&#125; value:&#123;&#125; offset:&#123;&#125;&quot;</span>,</span><br><span class="line">                records.get(<span class="number">0</span>).topic(),</span><br><span class="line">                records.get(<span class="number">0</span>).partition(),</span><br><span class="line">                records.get(<span class="number">0</span>).value(),</span><br><span class="line">                records.get(<span class="number">0</span>).offset());</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;消息转发：&quot;</span> + records.get(<span class="number">0</span>).value().toString();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>输出结果，如果结果不同，可以查看自己设置的延时时间及配置</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">2021-01-30 14:26:29 DEBUG (KafkaConsumer.java:122)- 消息转发 6：topic:test-topic_6 partition:0 value:这是一个简单的消息_6！ offset:10</span><br><span class="line">2021-01-30 14:26:29 DEBUG (KafkaConsumer.java:122)- 消息转发 6：topic:test-topic_6 partition:0 value:这是一个简单的消息_6！———————————————————————————————————————————————— offset:11</span><br><span class="line">2021-01-30 14:26:29 DEBUG (KafkaConsumer.java:112)- 批量消费-拦截长度小于20的字符串： value:消息转发：这是一个简单的消息_6！———————————————————————————————————————————————— </span><br></pre></td></tr></table></figure>

<h4 id="消费监听的起停"><a href="#消费监听的起停" class="headerlink" title="消费监听的起停"></a>消费监听的起停</h4><p>默认情况下，当消费者项目启动的时候，监听器就开始工作，监听消费发送到指定<code>topic</code>的消息， 那如果我们不想让监听器立即工作，想让它在我们指定的时间点开始工作，<br>或者在我们指定的时间点停止工作，使用 <code>KafkaListenerEndpointRegistry</code></p>
<p>实现：</p>
<ol>
<li>禁止监听器自启动;</li>
<li>延时开启，发送测试;</li>
<li>延时暂停，发送测试;</li>
<li>延时停止，发送测试;</li>
</ol>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Slf4j</span></span><br><span class="line"><span class="meta">@Component</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">KafkaConsumer</span> &#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 监听器容器工厂(设置禁止KafkaListener自启动)</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Bean</span></span><br><span class="line">    <span class="keyword">public</span> ConcurrentKafkaListenerContainerFactory <span class="title function_">delayContainerFactory</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="type">ConcurrentKafkaListenerContainerFactory</span> <span class="variable">container</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">ConcurrentKafkaListenerContainerFactory</span>();</span><br><span class="line">        container.setConsumerFactory(consumerFactory);</span><br><span class="line">        <span class="comment">/*禁止 KafkaListener 自启动*/</span></span><br><span class="line">        container.setAutoStartup(<span class="literal">false</span>);</span><br><span class="line">        <span class="keyword">return</span> container;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 消息监听的起停控制</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@KafkaListener(id = &quot;consumer7&quot;, groupId = &quot;consumer-group&quot;,</span></span><br><span class="line"><span class="meta">            topics = &#123;DefaultTestKafkaInitialConfiguration.TEST_TOPIC_7&#125;,</span></span><br><span class="line"><span class="meta">            containerFactory = &quot;delayContainerFactory&quot;)</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onMessage7</span><span class="params">(String record)</span> &#123;</span><br><span class="line">        log.debug(<span class="string">&quot;消息监听的起停控制-启动后可以接收到的消息： value:&#123;&#125; &quot;</span>, record);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Slf4j</span></span><br><span class="line"><span class="meta">@RunWith(SpringRunner.class)</span></span><br><span class="line"><span class="meta">@SpringBootTest(classes = start.DemoApplication.class)</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">KafkaTest</span> &#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 消息监听的起停控制 7</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">sendMessage7</span><span class="params">()</span> &#123;</span><br><span class="line">        kafkaService.sendMessage(DefaultTestKafkaInitialConfiguration.TEST_TOPIC_7, <span class="string">&quot;监听 7 默认未未启动，这是一个简单的消息_7！&quot;</span>);</span><br><span class="line">        <span class="comment">/*让启动后收到消息更明显*/</span></span><br><span class="line">        <span class="keyword">try</span> &#123; Thread.sleep(<span class="number">1000</span>); &#125; <span class="keyword">catch</span> (InterruptedException e) &#123; &#125;</span><br><span class="line">        <span class="keyword">if</span> (!registry.getListenerContainer(<span class="string">&quot;consumer7&quot;</span>).isRunning()) &#123;</span><br><span class="line">            log.debug(<span class="string">&quot;监听 7 还未启动，准备启动监听7&quot;</span>);</span><br><span class="line">            registry.getListenerContainer(<span class="string">&quot;consumer7&quot;</span>).start();</span><br><span class="line">        &#125;</span><br><span class="line">        log.debug(<span class="string">&quot;监听 7 启动&quot;</span>);</span><br><span class="line">        <span class="keyword">try</span> &#123; Thread.sleep(<span class="number">1000</span>); &#125; <span class="keyword">catch</span> (InterruptedException e) &#123; &#125;</span><br><span class="line">        kafkaService.sendMessage(DefaultTestKafkaInitialConfiguration.TEST_TOPIC_7, <span class="string">&quot;监听 7 已启动——————————！&quot;</span>);</span><br><span class="line">        <span class="keyword">try</span> &#123; Thread.sleep(<span class="number">1000</span>); &#125; <span class="keyword">catch</span> (InterruptedException e) &#123; &#125;</span><br><span class="line">        registry.getListenerContainer(<span class="string">&quot;consumer7&quot;</span>).pause();</span><br><span class="line">        log.debug(<span class="string">&quot;监听 7 暂停&quot;</span>);</span><br><span class="line">        <span class="keyword">try</span> &#123; Thread.sleep(<span class="number">5000</span>); &#125; <span class="keyword">catch</span> (InterruptedException e) &#123; &#125;</span><br><span class="line">        kafkaService.sendMessage(DefaultTestKafkaInitialConfiguration.TEST_TOPIC_7, <span class="string">&quot;监听 7 已暂停——————————————！&quot;</span>);</span><br><span class="line">        <span class="keyword">try</span> &#123; Thread.sleep(<span class="number">1000</span>); &#125; <span class="keyword">catch</span> (InterruptedException e) &#123; &#125;</span><br><span class="line">        registry.getListenerContainer(<span class="string">&quot;consumer7&quot;</span>).stop();</span><br><span class="line">        log.debug(<span class="string">&quot;监听 7 停止&quot;</span>);</span><br><span class="line">        <span class="keyword">try</span> &#123; Thread.sleep(<span class="number">1000</span>); &#125; <span class="keyword">catch</span> (InterruptedException e) &#123; &#125;</span><br><span class="line">        kafkaService.sendMessage(DefaultTestKafkaInitialConfiguration.TEST_TOPIC_7, <span class="string">&quot;监听 7 已停止——————————————！&quot;</span>);</span><br><span class="line">        log.debug(<span class="string">&quot;发送完成&quot;</span>);</span><br><span class="line">        <span class="keyword">try</span> &#123; Thread.sleep(<span class="number">5000</span>); &#125; <span class="keyword">catch</span> (InterruptedException e) &#123; &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>运行结果，</p>
<ol>
<li>暂停监听后，消息不会被接收（如果 <code>pause</code> 时，延迟1秒，可能消息会被接收到，与设想的不一样，可以调用 <code>isConsumerPaused</code>查看消息是否被暂停）</li>
<li>监听停止后，就不在接收消息。</li>
<li>虽然监听停止了，但因为 <code>kafka</code> 服务还在，所以消息是发送成功的。</li>
<li>监听停止，经过简单测试发现是立即执行的，监听暂停不是立即执行的。</li>
<li><code>pause()</code>方法在下一次<code>poll()</code>之前生效，而<code>resume()</code>方法在当前的<code>poll()</code>之后生效。当一个容器被暂停，它会继续拉取消费者，避免再均衡（如果组管理有使用），但是不会索取任何记录。</li>
</ol>
<p><code>pause</code>后延迟5秒的结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">2021-02-01 11:44:22 DEBUG (KafkaServiceImpl.java:30)- 消息发送成功：message:监听 7 默认未未启动，这是一个简单的消息_7！ topic:test-topic_7 partition:0 offset:11</span><br><span class="line">2021-02-01 11:44:23 DEBUG (KafkaTest.java:127)- 监听 7 还未启动，准备启动监听7</span><br><span class="line">2021-02-01 11:44:23 DEBUG (KafkaTest.java:130)- 监听 7 启动</span><br><span class="line">2021-02-01 11:44:24 DEBUG (KafkaServiceImpl.java:30)- 消息发送成功：message:监听 7 已启动——————————！ topic:test-topic_7 partition:0 offset:12</span><br><span class="line">2021-02-01 11:44:24 DEBUG (KafkaConsumer.java:151)- 消息监听的起停控制-启动后可以接收到的消息： value:监听 7 已启动——————————！ </span><br><span class="line">2021-02-01 11:44:25 DEBUG (KafkaTest.java:135)- 监听 7 暂停</span><br><span class="line">2021-02-01 11:44:30 DEBUG (KafkaServiceImpl.java:30)- 消息发送成功：message:监听 7 已暂停——————————————！ topic:test-topic_7 partition:0 offset:13</span><br><span class="line">2021-02-01 11:44:31 DEBUG (KafkaTest.java:140)- 监听 7 停止</span><br><span class="line">2021-02-01 11:44:32 DEBUG (KafkaTest.java:143)- 发送完成</span><br><span class="line">2021-02-01 11:44:32 DEBUG (KafkaServiceImpl.java:30)- 消息发送成功：message:监听 7 已停止——————————————！ topic:test-topic_7 partition:0 offset:14</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><code>pause</code>后延迟1秒的结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">2021-02-01 11:39:59 DEBUG (KafkaServiceImpl.java:30)- 消息发送成功：message:监听 7 默认未未启动，这是一个简单的消息_7！ topic:test-topic_7 partition:0 offset:7</span><br><span class="line">2021-02-01 11:40:00 DEBUG (KafkaTest.java:127)- 监听 7 还未启动，准备启动监听7</span><br><span class="line">2021-02-01 11:40:00 DEBUG (KafkaTest.java:130)- 监听 7 启动</span><br><span class="line">2021-02-01 11:40:01 DEBUG (KafkaServiceImpl.java:30)- 消息发送成功：message:监听 7 已启动——————————！ topic:test-topic_7 partition:0 offset:8</span><br><span class="line">2021-02-01 11:40:02 DEBUG (KafkaConsumer.java:151)- 消息监听的起停控制-启动后可以接收到的消息： value:监听 7 已启动——————————！ </span><br><span class="line">2021-02-01 11:40:02 DEBUG (KafkaTest.java:135)- 监听 7 暂停</span><br><span class="line">2021-02-01 11:40:03 DEBUG (KafkaServiceImpl.java:30)- 消息发送成功：message:监听 7 已暂停——————————————！ topic:test-topic_7 partition:0 offset:9</span><br><span class="line">2021-02-01 11:40:03 DEBUG (KafkaConsumer.java:151)- 消息监听的起停控制-启动后可以接收到的消息： value:监听 7 已暂停——————————————！ </span><br><span class="line">2021-02-01 11:40:05 DEBUG (KafkaTest.java:140)- 监听 7 停止</span><br><span class="line">2021-02-01 11:40:06 DEBUG (KafkaTest.java:144)- 发送完成</span><br><span class="line">2021-02-01 11:40:06 DEBUG (KafkaServiceImpl.java:30)- 消息发送成功：message:监听 7 已停止——————————————！ topic:test-topic_7 partition:0 offset:10</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="分组测试消费"><a href="#分组测试消费" class="headerlink" title="分组测试消费"></a>分组测试消费</h4><p>测试同一组、不同组消费同一个 <code>topic</code>的<code>partition</code></p>
<p>查看 <code>topic</code>：<code>bin/kafka-topics.sh  --zookeeper localhost:2181 --describe --topic test-topic_8</code></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Topic: test-topic_8	PartitionCount: 1	ReplicationFactor: 1	Configs: </span><br><span class="line">	Topic: test-topic_8	Partition: 0	Leader: 0	Replicas: 0	Isr: 0</span><br></pre></td></tr></table></figure>

<p>当前 <code>topic</code> 只有一个 <code>partition</code>。</p>
<p>测试前把当前 <code>kafka</code> 配置改为立即发送，并且设置不多发。</p>
<figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">spring:</span> </span><br><span class="line">  <span class="attr">kafka:</span> </span><br><span class="line">    <span class="comment"># 发送等待时间</span></span><br><span class="line">    <span class="attr">producer.properties.linger.ms:</span> <span class="number">0</span></span><br></pre></td></tr></table></figure>

<p>测试代码</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Slf4j</span></span><br><span class="line"><span class="meta">@Component</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">KafkaConsumer</span> &#123;</span><br><span class="line">    <span class="meta">@KafkaListener(id = &quot;consumer8&quot;, groupId = &quot;consumer-group8&quot;,</span></span><br><span class="line"><span class="meta">            topics = &#123;DefaultTestKafkaInitialConfiguration.TEST_TOPIC_8&#125;)</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onMessage8</span><span class="params">(String record)</span> &#123;</span><br><span class="line">        log.debug(<span class="string">&quot;分组测试消费：group8-consumer8 收到消息： value:&#123;&#125; &quot;</span>, record);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="meta">@KafkaListener(id = &quot;consumer8_1&quot;, groupId = &quot;consumer-group8&quot;,</span></span><br><span class="line"><span class="meta">            topics = &#123;DefaultTestKafkaInitialConfiguration.TEST_TOPIC_8&#125;)</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onMessage8_1</span><span class="params">(String record)</span> &#123;</span><br><span class="line">        log.debug(<span class="string">&quot;分组测试消费：group8-consumer8_1 收到消息： value:&#123;&#125; &quot;</span>, record);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="meta">@KafkaListener(id = &quot;consumer8_2&quot;, groupId = &quot;consumer-group8_2&quot;,</span></span><br><span class="line"><span class="meta">            topics = &#123;DefaultTestKafkaInitialConfiguration.TEST_TOPIC_8&#125;)</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onMessage8_2</span><span class="params">(String record)</span> &#123;</span><br><span class="line">        log.debug(<span class="string">&quot;分组测试消费：group8_2-consumer8_2 收到消息： value:&#123;&#125; &quot;</span>, record);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">@Slf4j</span></span><br><span class="line"><span class="meta">@RunWith(SpringRunner.class)</span></span><br><span class="line"><span class="meta">@SpringBootTest(classes = start.DemoApplication.class)</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">KafkaTest</span> &#123;   </span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 分组测试消费 8</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">sendMessage8</span><span class="params">()</span> &#123;</span><br><span class="line">        kafkaService.sendMessage(DefaultTestKafkaInitialConfiguration.TEST_TOPIC_8, <span class="string">&quot;这是一个简单的消息_8！&quot;</span>);</span><br><span class="line">        log.debug(<span class="string">&quot;发送完成&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>测试结果</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">2021-02-01 15:40:43 DEBUG (KafkaTest.java:172)- 发送完成</span><br><span class="line">2021-02-01 15:40:43 DEBUG (KafkaServiceImpl.java:30)- 消息发送成功：message:这是一个简单的消息_8！ topic:test-topic_8 partition:0 offset:0</span><br><span class="line">2021-02-01 15:40:43 DEBUG (KafkaConsumer.java:168)- 分组测试消费：group8-consumer8_1 收到消息： value:这是一个简单的消息_8！ </span><br><span class="line">2021-02-01 15:40:43 DEBUG (KafkaConsumer.java:176)- 分组测试消费：group8_2-consumer8_2 收到消息： value:这是一个简单的消息_8！ :0 offset:0</span><br></pre></td></tr></table></figure>
<p>也就是意味着<code>group8</code>中的两个监听只有一个监听到轮数据。</p>
<p>以下来自<a target="_blank" rel="noopener" href="https://www.jianshu.com/p/d3e963ff8b70">简书</a></p>
<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>kafka是一个分布式消息队列。具有高性能、持久化、多副本备份、横向扩展能力。生产者往队列里写消息，消费者从队列里取消息进行业务逻辑。一般在架构设计中起到解耦、削峰、异步处理的作用。</p>
<p>kafka对外使用topic的概念，生产者往topic里写消息，消费者从读消息。为了做到水平扩展，一个topic实际是由多个partition组成的，遇到瓶颈时，可以通过增加partition的数量来进行横向扩容。单个parition内是保证消息有序。</p>
<p>每新写一条消息，kafka就是在对应的文件append写，所以性能非常高。</p>
<p>kafka的总体数据流是这样的：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/2835676-f378607bc841309a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1200/format/webp" alt="img"></p>
<p>kafka data flow</p>
<p>大概用法就是，Producers往Brokers里面的指定Topic中写消息，Consumers从Brokers里面拉去指定Topic的消息，然后进行业务处理。<br> 图中有两个topic，topic 0有两个partition，topic 1有一个partition，三副本备份。可以看到consumer gourp 1中的consumer 2没有分到partition处理，这是有可能出现的，下面会讲到。</p>
<p>关于broker、topics、partitions的一些元信息用zk来存，监控和路由啥的也都会用到zk。</p>
<hr>
<h2 id="生产"><a href="#生产" class="headerlink" title="生产"></a>生产</h2><p>基本流程是这样的：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/2835676-66af39d6c3e1f769.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1200/format/webp" alt="img"></p>
<p>kafka sdk product flow.png</p>
<p>创建一条记录，记录中一个要指定对应的topic和value，key和partition可选。 先序列化，然后按照topic和partition，放进对应的发送队列中。kafka produce都是批量请求，会积攒一批，然后一起发送，不是调send()就进行立刻进行网络发包。<br> 如果partition没填，那么情况会是这样的：</p>
<ol>
<li>key有填<br> 按照key进行哈希，相同key去一个partition。（如果扩展了partition的数量那么就不能保证了）</li>
<li>key没填<br> round-robin来选partition</li>
</ol>
<p>这些要发往同一个partition的请求按照配置，攒一波，然后由一个单独的线程一次性发过去。</p>
<h3 id="API"><a href="#API" class="headerlink" title="API"></a>API</h3><p>有high level api，替我们把很多事情都干了，offset，路由啥都替我们干了，用以来很简单。<br> 还有simple api，offset啥的都是要我们自己记录。</p>
<h3 id="partition"><a href="#partition" class="headerlink" title="partition"></a>partition</h3><p>当存在多副本的情况下，会尽量把多个副本，分配到不同的broker上。<strong>kafka会为partition选出一个leader，之后所有该partition的请求，实际操作的都是leader，然后再同步到其他的follower。</strong>当一个broker歇菜后，所有leader在该broker上的partition都会重新选举，选出一个leader。（这里不像分布式文件存储系统那样会自动进行复制保持副本数）</p>
<p>然后这里就涉及两个细节：怎么分配partition，怎么选leader。</p>
<p><strong>关于partition的分配，还有leader的选举，总得有个执行者。在kafka中，这个执行者就叫controller。</strong>kafka使用zk在broker中选出一个controller，用于partition分配和leader选举。</p>
<h4 id="partition的分配"><a href="#partition的分配" class="headerlink" title="partition的分配"></a>partition的分配</h4><ol>
<li>将所有Broker（假设共n个Broker）和待分配的Partition排序</li>
<li>将第i个Partition分配到第（i mod n）个Broker上 （这个就是leader）</li>
<li>将第i个Partition的第j个Replica分配到第（(i + j) mode n）个Broker上</li>
</ol>
<h4 id="leader容灾"><a href="#leader容灾" class="headerlink" title="leader容灾"></a>leader容灾</h4><p>controller会在Zookeeper的&#x2F;brokers&#x2F;ids节点上注册Watch，一旦有broker宕机，它就能知道。当broker宕机后，controller就会给受到影响的partition选出新leader。controller从zk的&#x2F;brokers&#x2F;topics&#x2F;[topic]&#x2F;partitions&#x2F;[partition]&#x2F;state中，读取对应partition的ISR（in-sync replica已同步的副本）列表，选一个出来做leader。<br> 选出leader后，更新zk，然后发送LeaderAndISRRequest给受影响的broker，让它们改变知道这事。为什么这里不是使用zk通知，而是直接给broker发送rpc请求，我的理解可能是这样做zk有性能问题吧。</p>
<p>如果ISR列表是空，那么会根据配置，随便选一个replica做leader，或者干脆这个partition就是歇菜。如果ISR列表的有机器，但是也歇菜了，那么还可以等ISR的机器活过来。</p>
<h4 id="多副本同步"><a href="#多副本同步" class="headerlink" title="多副本同步"></a>多副本同步</h4><p>这里的策略，服务端这边的处理是follower从leader批量拉取数据来同步。但是具体的可靠性，是由生产者来决定的。<br> 生产者生产消息的时候，通过request.required.acks参数来设置数据的可靠性。</p>
<table>
<thead>
<tr>
<th>acks</th>
<th>what happen</th>
</tr>
</thead>
<tbody><tr>
<td>0</td>
<td>which means that the producer never waits for an acknowledgement from the broker.发过去就完事了，不关心broker是否处理成功，可能丢数据。</td>
</tr>
<tr>
<td>1</td>
<td>which means that the producer gets an acknowledgement after the leader replica has received the data. 当写Leader成功后就返回,其他的replica都是通过fetcher去同步的,所以kafka是异步写，主备切换可能丢数据。</td>
</tr>
<tr>
<td>-1</td>
<td>which means that the producer gets an acknowledgement after all in-sync replicas have received the data. 要等到isr里所有机器同步成功，才能返回成功，延时取决于最慢的机器。强一致，不会丢数据。</td>
</tr>
</tbody></table>
<p>在acks&#x3D;-1的时候，如果ISR少于min.insync.replicas指定的数目，那么就会返回不可用。</p>
<p>这里ISR列表中的机器是会变化的，根据配置replica.lag.time.max.ms，多久没同步，就会从ISR列表中剔除。以前还有根据落后多少条消息就踢出ISR，在1.0版本后就去掉了，因为这个值很难取，在高峰的时候很容易出现节点不断的进出ISR列表。</p>
<p>从ISA中选出leader后，follower会从把自己日志中上一个高水位后面的记录去掉，然后去和leader拿新的数据。因为新的leader选出来后，follower上面的数据，可能比新leader多，所以要截取。这里高水位的意思，对于partition和leader，就是所有ISR中都有的最新一条记录。消费者最多只能读到高水位；</p>
<p>从leader的角度来说高水位的更新会延迟一轮，例如写入了一条新消息，ISR中的broker都fetch到了，但是ISR中的broker只有在下一轮的fetch中才能告诉leader。</p>
<p>也正是由于这个高水位延迟一轮，在一些情况下，kafka会出现丢数据和主备数据不一致的情况，0.11开始，使用leader epoch来代替高水位。（<a target="_blank" rel="noopener" href="https://link.jianshu.com/?t=https://cwiki.apache.org/confluence/display/KAFKA/KIP-101+-+Alter+Replication+Protocol+to+use+Leader+Epoch+rather+than+High+Watermark+for+Truncation%23KIP-101-AlterReplicationProtocoltouseLeaderEpochratherthanHighWatermarkforTruncation-Scenario1:HighWatermarkTruncationfollowedbyImmediateLeaderElection">https://cwiki.apache.org/confluence/display/KAFKA/KIP-101+-+Alter+Replication+Protocol+to+use+Leader+Epoch+rather+than+High+Watermark+for+Truncation#KIP-101-AlterReplicationProtocoltouseLeaderEpochratherthanHighWatermarkforTruncation-Scenario1:HighWatermarkTruncationfollowedbyImmediateLeaderElection</a>）</p>
<p>*<strong>思考：*</strong><br> 当acks&#x3D;-1时</p>
<ol>
<li>是follwers都来fetch就返回成功，还是等follwers第二轮fetch？</li>
<li>leader已经写入本地，但是ISR中有些机器失败，那么怎么处理呢？</li>
</ol>
<hr>
<h2 id="消费"><a href="#消费" class="headerlink" title="消费"></a>消费</h2><p>订阅topic是以一个消费组来订阅的，一个消费组里面可以有多个消费者。同一个消费组中的两个消费者，不会同时消费一个partition。换句话来说，<strong>就是一个partition，只能被消费组里的一个消费者消费</strong>，但是可以同时被多个消费组消费。因此，如果消费组内的消费者如果比partition多的话，那么就会有个别消费者一直空闲。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/2835676-15c32e096cdbdabb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1200/format/webp" alt="img"></p>
<p>untitled_page.png</p>
<h3 id="API-1"><a href="#API-1" class="headerlink" title="API"></a>API</h3><p>订阅topic时，可以用正则表达式，如果有新topic匹配上，那能自动订阅上。</p>
<h3 id="offset的保存"><a href="#offset的保存" class="headerlink" title="offset的保存"></a>offset的保存</h3><p>一个消费组消费partition，需要保存offset记录消费到哪，以前保存在zk中，由于zk的写性能不好，以前的解决方法都是consumer每隔一分钟上报一次。这里zk的性能严重影响了消费的速度，而且很容易出现重复消费。<br> 在0.10版本后，kafka把这个offset的保存，从zk总剥离，保存在一个名叫__consumeroffsets topic的topic中。写进消息的key由groupid、topic、partition组成，value是偏移量offset。topic配置的清理策略是compact。总是保留最新的key，其余删掉。一般情况下，每个key的offset都是缓存在内存中，查询的时候不用遍历partition，如果没有缓存，第一次就会遍历partition建立缓存，然后查询返回。</p>
<p>确定consumer group位移信息写入__consumers_offsets的哪个partition，具体计算公式：</p>
<figure class="highlight swift"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">__consumers_offsets partition <span class="operator">=</span></span><br><span class="line">           <span class="type">Math</span>.abs(groupId.hashCode() <span class="operator">%</span> groupMetadataTopicPartitionCount)   </span><br><span class="line"><span class="comment">//groupMetadataTopicPartitionCount由offsets.topic.num.partitions指定，默认是50个分区。</span></span><br></pre></td></tr></table></figure>

<p>*<strong>思考：*</strong><br> 如果正在跑的服务，修改了offsets.topic.num.partitions，那么offset的保存是不是就乱套了？</p>
<h3 id="分配partition–reblance"><a href="#分配partition–reblance" class="headerlink" title="分配partition–reblance"></a>分配partition–reblance</h3><p>生产过程中broker要分配partition，消费过程这里，也要分配partition给消费者。类似broker中选了一个controller出来，消费也要从broker中选一个coordinator，用于分配partition。<br>下面从顶向下，分别阐述一下</p>
<ul>
<li>怎么选coordinator。</li>
<li>交互流程。</li>
<li>reblance的流程。</li>
</ul>
<h4 id="选coordinator"><a href="#选coordinator" class="headerlink" title="选coordinator"></a>选coordinator</h4><ul>
<li>看offset保存在那个partition</li>
<li>该partition leader所在的broker就是被选定的coordinator</li>
</ul>
<p>这里我们可以看到，consumer group的coordinator，和保存consumer group offset的partition leader是同一台机器。</p>
<h4 id="交互流程"><a href="#交互流程" class="headerlink" title="交互流程"></a>交互流程</h4><p>把coordinator选出来之后，就是要分配了<br>整个流程是这样的：</p>
<ol>
<li>consumer启动、或者coordinator宕机了，consumer会任意请求一个broker，发送ConsumerMetadataRequest请求，broker会按照上面说的方法，选出这个consumer对应coordinator的地址。<br>2.consumer 发送heartbeat请求给coordinator，返回IllegalGeneration的话，就说明consumer的信息是旧的了，需要重新加入进来，进行reblance。返回成功，那么consumer就从上次分配的partition中继续执行。</li>
</ol>
<h4 id="reblance流程"><a href="#reblance流程" class="headerlink" title="reblance流程"></a>reblance流程</h4><ol>
<li>consumer给coordinator发送JoinGroupRequest请求。</li>
<li>这时其他consumer发heartbeat请求过来时，coordinator会告诉他们，要reblance了。</li>
<li>其他consumer发送JoinGroupRequest请求。</li>
<li>所有记录在册的consumer都发了JoinGroupRequest请求之后，coordinator就会在这里consumer中随便选一个leader。然后回JoinGroupRespone，这会告诉consumer你是follower还是leader，对于leader，还会把follower的信息带给它，让它根据这些信息去分配partition</li>
<li>consumer向coordinator发送SyncGroupRequest，其中leader的SyncGroupRequest会包含分配的情况。</li>
<li>coordinator回包，把分配的情况告诉consumer，包括leader。</li>
</ol>
<p>当partition或者消费者的数量发生变化时，都得进行 <code>reblance</code>。<br>列举一下会 <code>reblance</code> 的情况：</p>
<ul>
<li>增加 <code>partition</code></li>
<li>增加消费者</li>
<li>消费者主动关闭</li>
<li>消费者宕机了</li>
<li><code>coordinator</code> 自己也宕机了</li>
</ul>
<h2 id="消息投递语义"><a href="#消息投递语义" class="headerlink" title="消息投递语义"></a>消息投递语义</h2><p>kafka支持3种消息投递语义<br> At most once：最多一次，消息可能会丢失，但不会重复<br> At least once：最少一次，消息不会丢失，可能会重复<br> Exactly once：只且一次，消息不丢失不重复，只且消费一次（0.11中实现，仅限于下游也是kafka）</p>
<p>在业务中，常常都是使用At least once的模型，如果需要可重入的话，往往是业务自己实现。</p>
<h3 id="At-least-once"><a href="#At-least-once" class="headerlink" title="At least once"></a>At least once</h3><p>先获取数据，再进行业务处理，业务处理成功后commit offset。<br> 1、生产者生产消息异常，消息是否成功写入不确定，重做，可能写入重复的消息<br> 2、消费者处理消息，业务处理成功后，更新offset失败，消费者重启的话，会重复消费</p>
<h3 id="At-most-once"><a href="#At-most-once" class="headerlink" title="At most once"></a>At most once</h3><p>先获取数据，再commit offset，最后进行业务处理。<br> 1、生产者生产消息异常，不管，生产下一个消息，消息就丢了<br> 2、消费者处理消息，先更新offset，再做业务处理，做业务处理失败，消费者重启，消息就丢了</p>
<h3 id="Exactly-once"><a href="#Exactly-once" class="headerlink" title="Exactly once"></a>Exactly once</h3><p>思路是这样的，首先要保证消息不丢，再去保证不重复。所以盯着At least once的原因来搞。 首先想出来的：</p>
<ol>
<li>生产者重做导致重复写入消息—-生产保证幂等性</li>
<li>消费者重复消费—消灭重复消费，或者业务接口保证幂等性重复消费也没问题</li>
</ol>
<p><strong>由于业务接口是否幂等，不是kafka能保证的，所以kafka这里提供的exactly once是有限制的，消费者的下游也必须是kafka。</strong>所以一下讨论的，没特殊说明，消费者的下游系统都是kafka（注:使用kafka conector，它对部分系统做了适配，实现了exactly once）。</p>
<p>生产者幂等性好做，没啥问题。</p>
<p>解决重复消费有两个方法：</p>
<ol>
<li>下游系统保证幂等性，重复消费也不会导致多条记录。</li>
<li>把commit offset和业务处理绑定成一个事务。</li>
</ol>
<p>本来exactly once实现第1点就ok了。</p>
<p>但是在一些使用场景下，我们的数据源可能是多个topic，处理后输出到多个topic，这时我们会希望输出时要么全部成功，要么全部失败。<strong>这就需要实现事务性。</strong>既然要做事务，那么干脆<strong>把重复消费的问题从根源上解决，把commit offset和输出到其他topic绑定成一个事务。</strong></p>
<h4 id="生产幂等性"><a href="#生产幂等性" class="headerlink" title="生产幂等性"></a>生产幂等性</h4><p>思路是这样的，为每个producer分配一个pid，作为该producer的唯一标识。producer会为每一个&lt;topic,partition&gt;维护一个单调递增的seq。类似的，broker也会为每个&lt;pid,topic,partition&gt;记录下最新的seq。当req_seq &#x3D;&#x3D; broker_seq+1时，broker才会接受该消息。因为：</p>
<ol>
<li><p>消息的seq比broker的seq大超过时，说明中间有数据还没写入，即乱序了。</p>
</li>
<li><p>消息的seq不比broker的seq小，那么说明该消息已被保存。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/2835676-6cdca2f01bdd7e4b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/496/format/webp" alt="img"></p>
<p>解决重复生产</p>
</li>
</ol>
<h4 id="事务性-x2F-原子性广播"><a href="#事务性-x2F-原子性广播" class="headerlink" title="事务性&#x2F;原子性广播"></a>事务性&#x2F;原子性广播</h4><p>场景是这样的：</p>
<ol>
<li>先从多个源topic中获取数据。</li>
<li>做业务处理，写到下游的多个目的topic。</li>
<li>更新多个源topic的offset。</li>
</ol>
<p>其中第2、3点作为一个事务，要么全成功，要么全失败。这里得益与offset实际上是用特殊的topic去保存，这两点都归一为写多个topic的事务性处理。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/2835676-cb4882e8e69cc9b8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/864/format/webp" alt="img"></p>
<p>基本思路是这样的：<br> 引入tid（transaction id），和pid不同，这个id是应用程序提供的，用于标识事务，和producer是谁并没关系。就是任何producer都可以使用这个tid去做事务，这样进行到一半就死掉的事务，可以由另一个producer去恢复。<br> 同时为了记录事务的状态，类似对offset的处理，引入transaction coordinator用于记录transaction log。在集群中会有多个transaction coordinator，每个tid对应唯一一个transaction coordinator。<br> 注：transaction log删除策略是compact，已完成的事务会标记成null，compact后不保留。</p>
<p>做事务时，先标记开启事务，写入数据，全部成功就在transaction log中记录为prepare commit状态，否则写入prepare abort的状态。之后再去给每个相关的partition写入一条marker（commit或者abort）消息，标记这个事务的message可以被读取或已经废弃。成功后在transaction log记录下commit&#x2F;abort状态，至此事务结束。</p>
<p>数据流：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/2835676-3ddc2841526aeb71.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/720/format/webp" alt="img"></p>
<p>Kafka Transactions Data Flow.png</p>
<ol>
<li><p>首先使用tid请求任意一个broker（代码中写的是负载最小的broker），找到对应的transaction coordinator。</p>
</li>
<li><p>请求transaction coordinator获取到对应的pid，和pid对应的epoch，这个epoch用于防止僵死进程复活导致消息错乱，当消息的epoch比当前维护的epoch小时，拒绝掉。tid和pid有一一对应的关系，这样对于同一个tid会返回相同的pid。</p>
</li>
<li><p>client先请求transaction coordinator记录&lt;topic,partition&gt;的事务状态，初始状态是BEGIN，如果是该事务中第一个到达的&lt;topic,partition&gt;，同时会对事务进行计时；client输出数据到相关的partition中；client再请求transaction coordinator记录offset的&lt;topic,partition&gt;事务状态；client发送offset commit到对应offset partition。</p>
</li>
<li><p>client发送commit请求，transaction coordinator记录prepare commit&#x2F;abort，然后发送marker给相关的partition。全部成功后，记录commit&#x2F;abort的状态，最后这个记录不需要等待其他replica的ack，因为prepare不丢就能保证最终的正确性了。</p>
</li>
</ol>
<p>这里prepare的状态主要是用于事务恢复，例如给相关的partition发送控制消息，没发完就宕机了，备机起来后，producer发送请求获取pid时，会把未完成的事务接着完成。</p>
<p>当partition中写入commit的marker后，相关的消息就可被读取。所以kafka事务在prepare commit到commit这个时间段内，消息是逐渐可见的，而不是同一时刻可见。</p>
<p>详细细节可看：<a target="_blank" rel="noopener" href="https://link.jianshu.com/?t=https://cwiki.apache.org/confluence/display/KAFKA/KIP-98+-+Exactly+Once+Delivery+and+Transactional+Messaging%23KIP-98-ExactlyOnceDeliveryandTransactionalMessaging-TransactionalGuarantees">https://cwiki.apache.org/confluence/display/KAFKA/KIP-98+-+Exactly+Once+Delivery+and+Transactional+Messaging#KIP-98-ExactlyOnceDeliveryandTransactionalMessaging-TransactionalGuarantees</a></p>
<h4 id="消费事务"><a href="#消费事务" class="headerlink" title="消费事务"></a>消费事务</h4><p>前面都是从生产的角度看待事务。还需要从消费的角度去考虑一些问题。<br> 消费时，partition中会存在一些消息处于未commit状态，即业务方应该看不到的消息，需要过滤这些消息不让业务看到，kafka选择在消费者进程中进行过来，而不是在broker中过滤，主要考虑的还是性能。kafka高性能的一个关键点是zero copy，如果需要在broker中过滤，那么势必需要读取消息内容到内存，就会失去zero copy的特性。</p>
<hr>
<h2 id="文件组织"><a href="#文件组织" class="headerlink" title="文件组织"></a>文件组织</h2><p>kafka的数据，实际上是以文件的形式存储在文件系统的。topic下有partition，partition下有segment，segment是实际的一个个文件，topic和partition都是抽象概念。</p>
<p>在目录&#x2F;${topicName}-{$partitionid}&#x2F;下，存储着实际的log文件（即segment），还有对应的索引文件。</p>
<p>每个segment文件大小相等，文件名以这个segment中最小的offset命名，文件扩展名是.log；segment对应的索引的文件名字一样，扩展名是.index。有两个index文件，一个是offset index用于按offset去查message，一个是time index用于按照时间去查，其实这里可以优化合到一起，下面只说offset index。总体的组织是这样的：</p>
<p><img src="https://upload-images.jianshu.io/upload_images/2835676-3d067d57adff7834.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1200/format/webp" alt="img"></p>
<p>kafka 文件组织.png</p>
<p>为了减少索引文件的大小，降低空间使用，方便直接加载进内存中，这里的索引使用稀疏矩阵，不会每一个message都记录下具体位置，而是每隔一定的字节数，再建立一条索引。 索引包含两部分，分别是baseOffset，还有position。</p>
<p>baseOffset：意思是这条索引对应segment文件中的第几条message。这样做方便使用数值压缩算法来节省空间。例如kafka使用的是varint。</p>
<p>position：在segment中的绝对位置。</p>
<p>查找offset对应的记录时，会先用二分法，找出对应的offset在哪个segment中，然后使用索引，在定位出offset在segment中的大概位置，再遍历查找message。</p>
<hr>
<h2 id="常用配置项"><a href="#常用配置项" class="headerlink" title="常用配置项"></a>常用配置项</h2><h3 id="broker配置"><a href="#broker配置" class="headerlink" title="broker配置"></a>broker配置</h3><table>
<thead>
<tr>
<th>配置项</th>
<th>作用</th>
</tr>
</thead>
<tbody><tr>
<td>broker.id</td>
<td>broker的唯一标识</td>
</tr>
<tr>
<td>auto.create.topics.auto</td>
<td>设置成true，就是遇到没有的topic自动创建topic。</td>
</tr>
<tr>
<td>log.dirs</td>
<td>log的目录数，目录里面放partition，当生成新的partition时，会挑目录里partition数最少的目录放。</td>
</tr>
</tbody></table>
<h3 id="topic配置"><a href="#topic配置" class="headerlink" title="topic配置"></a>topic配置</h3><table>
<thead>
<tr>
<th>配置项</th>
<th>作用</th>
</tr>
</thead>
<tbody><tr>
<td>num.partitions</td>
<td>新建一个topic，会有几个partition。</td>
</tr>
<tr>
<td>log.retention.ms</td>
<td>对应的还有minutes，hours的单位。日志保留时间，因为删除是文件维度而不是消息维度，看的是日志文件的mtime。</td>
</tr>
<tr>
<td>log.retention.bytes</td>
<td>partion最大的容量，超过就清理老的。注意这个是partion维度，就是说如果你的topic有8个partition，配置1G，那么平均分配下，topic理论最大值8G。</td>
</tr>
<tr>
<td>log.segment.bytes</td>
<td>一个segment的大小。超过了就滚动。</td>
</tr>
<tr>
<td>log.segment.ms</td>
<td>一个segment的打开时间，超过了就滚动。</td>
</tr>
<tr>
<td>message.max.bytes</td>
<td>message最大多大</td>
</tr>
</tbody></table>
<p>关于日志清理，默认当前正在写的日志，是怎么也不会清理掉的。<br> 还有0.10之前的版本，时间看的是日志文件的mtime，但这个指是不准确的，有可能文件被touch一下，mtime就变了。因此在0.10版本开始，改为使用该文件最新一条消息的时间来判断。<br> 按大小清理这里也要注意，Kafka在定时任务中尝试比较当前日志量总大小是否超过阈值至少一个日志段的大小。如果超过但是没超过一个日志段，那么就不会删除。</p>
<p>本文地址： <a href="https://github.com/maxzhao-it/blog/post/50825/">https://github.com/maxzhao-it/blog/post/50825/</a> </p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://github.com/maxzhao-it/blog/post/4175/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/blog/uploads/images/avatar.jpg">
      <meta itemprop="name" content="赵联胜">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="maxzhao">
      <meta itemprop="description" content="小码农赵联胜的博客,从遇到Java到爱上Java，工作之余学习Java生态圈中的各种技术。">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | maxzhao">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/blog/post/4175/" class="post-title-link" itemprop="url">Kafka安装配置</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2022-09-16 17:31:12" itemprop="dateModified" datetime="2022-09-16T17:31:12+08:00">2022-09-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/blog/categories/DevelopTools/" itemprop="url" rel="index"><span itemprop="name">DevelopTools</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/blog/categories/DevelopTools/kafka/" itemprop="url" rel="index"><span itemprop="name">kafka</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/qingyunzong/p/9004509.html">kafka介绍1</a></p>
<p><a target="_blank" rel="noopener" href="https://kafka.apachecn.org/quickstart.html">kafka.apachecn介绍</a></p>
<h2 id="一、安装"><a href="#一、安装" class="headerlink" title="一、安装"></a>一、安装</h2><p><code>kafka</code> 可以通过<a target="_blank" rel="noopener" href="https://kafka.apache.org/downloads">官网下载</a></p>
<p><code>kafka</code> 根据Scala版本不同，又分为多个版本，我不需要使用<code>Scala</code>，所以就下载官方推荐版本<code>kafka_2.13-2.7.0.tgz</code>。</p>
<p> 解压到<code>opt</code>目录下</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">wget https://downloads.apache.org/kafka/3.2.1/kafka_2.13-3.2.1.tgz</span><br><span class="line">tar -xzf kafka_2.13-3.2.1.tgz -C ~/</span><br><span class="line"><span class="built_in">cd</span> ~/</span><br><span class="line"><span class="comment"># 为了使用方便，可以创建软链接`kafka`</span></span><br><span class="line"><span class="built_in">ln</span> -s kafka_2.13-3.2.1 kafka</span><br><span class="line"><span class="comment">#mv kafka_2.13-2.7.0 kafka</span></span><br><span class="line"><span class="built_in">cd</span> kafka</span><br></pre></td></tr></table></figure>

<p><code>kafka</code>的安装需要依赖<code>Zookeeper</code>。</p>
<h2 id="二、默认的Zookeeper"><a href="#二、默认的Zookeeper" class="headerlink" title="二、默认的Zookeeper"></a>二、默认的<code>Zookeeper</code></h2><p><code>kafka</code>自带的<code>Zookeeper</code>程序使用<code>bin/zookeeper-server-start.sh</code>，以及<code>bin/zookeeper-server-stop.sh</code>来启动和停止<code>Zookeeper</code>。</p>
<p><code>Zookeeper</code>的配制文件是<code>config/zookeeper.properties</code>，可以修改其中的参数</p>
<h3 id="（1）启动Zookeeper"><a href="#（1）启动Zookeeper" class="headerlink" title="（1）启动Zookeeper"></a>（1）启动<code>Zookeeper</code></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 修改默认端口</span></span><br><span class="line">sed -i <span class="string">&#x27;s?clientPort=2181?clientPort=42181?&#x27;</span> ~/kafka/config/zookeeper.properties</span><br><span class="line"><span class="built_in">cat</span> ~/kafka/config/zookeeper.properties</span><br><span class="line">~/kafka/bin/zookeeper-server-start.sh -daemon ~/kafka/config/zookeeper.properties</span><br><span class="line"><span class="built_in">tail</span> -f ~/kafka/logs/zookeeper.out</span><br></pre></td></tr></table></figure>

<ul>
<li><code>-daemon</code>参数，可以在后台启动<code>Zookeeper</code>，输出的信息在保存在执行目录的<code>logs/zookeeper.out</code>文件中。</li>
</ul>
<blockquote>
<p>问题：对于小内存的服务器，启动时有可能会出现如下错误。</p>
<p><code>os::commit_memory(0x00000000e0000000, 536870912, 0) failed; error=&#39;Not enough space&#39; (errno=12)</code></p>
<p>可以通过修改<code>bin/zookeeper-server-start.sh</code>中的参数，来减少内存的使用，将<code>-Xmx512M -Xms512M</code>改小。</p>
</blockquote>
<h3 id="（2）关闭Zookeeper"><a href="#（2）关闭Zookeeper" class="headerlink" title="（2）关闭Zookeeper"></a>（2）关闭<code>Zookeeper</code></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">~/kafka/bin/zookeeper-server-stop.sh -daemon ~/kafka/config/zookeeper.properties</span><br></pre></td></tr></table></figure>

<h2 id="三、Kafka配置"><a href="#三、Kafka配置" class="headerlink" title="三、Kafka配置"></a>三、Kafka配置</h2><p><code>kafka</code>的配置文件在<code>config/server.properties</code>文件中，主要修改参数如下，更具体的参数说明以后再整理下。</p>
<ol>
<li><code>broker.id</code>是<code>kafka broker</code>的编号，集群里每个<code>broker</code>的<code>id</code>需不同。默认从0开始。</li>
<li><code>listeners</code>是监听地址，需要提供外网服务的话，要设置本地的<code>IP</code>地址</li>
<li><code>log.dirs</code>是日志目录，需要设置</li>
<li>设置<code>Zookeeper</code>集群地址，我是在同一个服务器上搭建了<code>kafka</code>和<code>Zookeeper</code>，所以填的本地地址</li>
<li><code>num.partitions</code> 为新建<code>Topic</code>的默认<code>Partition</code>数量，<code>partition</code>数量提升，一定程度上可以提升并发性，数值应该小于等于<code>broker</code>的数量</li>
<li>因为要创建<code>kafka</code>集群，所以<code>kafka</code>的所有文件都复制两份，配置文件做相应的修改，尤其是<code>brokerid</code>、<code>IP</code>地址和<code>log.dirs</code>。分别创建软链接<code>kafka1</code>和<code>kafka2</code>。</li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 集群里每个`broker`的`id`需不同。默认从0开始。</span></span><br><span class="line">broker.id=0</span><br><span class="line"><span class="comment">############################# Socket Server Settings #############################</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># The address the socket server listens on. It will get the value returned from </span></span><br><span class="line"><span class="comment"># java.net.InetAddress.getCanonicalHostName() if not configured.</span></span><br><span class="line"><span class="comment">#   FORMAT:</span></span><br><span class="line"><span class="comment">#     listeners = listener_name://host_name:port</span></span><br><span class="line"><span class="comment">#   EXAMPLE:</span></span><br><span class="line"><span class="comment">#     listeners = PLAINTEXT://your.host.name:9092</span></span><br><span class="line"><span class="comment"># 监听的地址和端口，当前监听本机所有</span></span><br><span class="line">listeners=EXTERNAL://0.0.0.0:49092</span><br><span class="line"><span class="comment"># 主机地址</span></span><br><span class="line">advertised.listeners=PLAINTEXT://192.168.14.123:49092</span><br><span class="line"><span class="comment"># Maps listener names to security protocols, the default is for them to be the same. See the config documentation for more details</span></span><br><span class="line"><span class="comment">#listener.security.protocol.map=PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL</span></span><br><span class="line"><span class="comment">############################# Log Basics #############################</span></span><br><span class="line"><span class="comment"># A comma separated list of directories under which to store log files</span></span><br><span class="line">log.dirs=/tmp/kafka-logs</span><br><span class="line"><span class="comment"># 分区</span></span><br><span class="line">num.partitions=1 </span><br><span class="line"><span class="comment">############################# Zookeeper #############################</span></span><br><span class="line">zookeeper.connect=localhost:42181</span><br></pre></td></tr></table></figure>



<h2 id="四、启动及停止Kafka"><a href="#四、启动及停止Kafka" class="headerlink" title="四、启动及停止Kafka"></a>四、启动及停止Kafka</h2><h3 id="（1）启动kafka"><a href="#（1）启动kafka" class="headerlink" title="（1）启动kafka"></a>（1）启动<code>kafka</code></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">~/kafka/bin/kafka-server-start.sh -daemon ~/kafka/config/server.properties</span><br><span class="line"><span class="built_in">tail</span> -f ~/kafka/logs/kafkaServer.out</span><br></pre></td></tr></table></figure>

<p> <code>-daemon</code> 参数会将任务转入后台运行，输出日志信息将写入日志文件，查看 <code>logs/kafkaServer.out</code>，如果结尾输同started说明启动成功。</p>
<p>也可以用<code>ps -ef|grep kafka</code>命令，看有没有<code>kafka</code>的进程</p>
<blockquote>
<p><code>kafka</code>默认的 <code>xmx xms</code>都是 <code>1G</code>，  对于小内存的服务器，启动时有可能会出现如下错误。</p>
<p><code>os::commit_memory(0x00000000e0000000, 536870912, 0) failed; error=&#39;Not enough space&#39; (errno=12)</code></p>
<p>可以通过修改<code>bin/kafka-server-start.sh</code>中的参数，来减少内存的使用，将配置改为<code>-Xmx256M -Xms64M</code>。</p>
</blockquote>
<h3 id="（2）停止kafka"><a href="#（2）停止kafka" class="headerlink" title="（2）停止kafka"></a>（2）停止<code>kafka</code></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">~/kafka/bin/kafka-server-stop.sh -daemon ~/kafka/config/server.properties</span><br></pre></td></tr></table></figure>

<h2 id="五、测试"><a href="#五、测试" class="headerlink" title="五、测试"></a>五、测试</h2><p>前提：<code>kafka</code>和<code>Zookeeper</code>已启动完成</p>
<h3 id="1、创建topic"><a href="#1、创建topic" class="headerlink" title="1、创建topic"></a>1、创建topic</h3><p>创建一个名为 <code>test</code> 的<code>Topic</code>，只有一个备份（<code>replication-facto</code>）和一个分区（<code>partitions</code>）。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">~/kafka/bin/kafka-topics.sh --create --bootstrap-server 192.168.14.123:49092 --replication-factor 1 --partitions 1 --topic <span class="built_in">test</span></span><br></pre></td></tr></table></figure>

<p>成功提示 <code>Created topic test.</code></p>
<h3 id="2、查看主题"><a href="#2、查看主题" class="headerlink" title="2、查看主题"></a>2、查看主题</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">~/kafka/bin/kafka-topics.sh --list --bootstrap-server 192.168.14.123:49092</span><br></pre></td></tr></table></figure>

<p>成功提示</p>
<p><code>test</code></p>
<h3 id="3、发送消息"><a href="#3、发送消息" class="headerlink" title="3、发送消息"></a>3、发送消息</h3><p><code>Kafka</code>提供了一个命令行的工具，可以从输入文件或者命令行中读取消息并发送给<code>Kafka</code>集群。每一行是一条消息。<br>运行<code>producer</code>（生产者）,然后在控制台输入几条消息到服务器。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">~/kafka/bin/kafka-console-producer.sh --broker-list 192.168.14.123:49092 --topic <span class="built_in">test</span></span><br><span class="line">First message;</span><br><span class="line">Second mssage;</span><br></pre></td></tr></table></figure>

<h3 id="4、接收消息"><a href="#4、接收消息" class="headerlink" title="4、接收消息"></a>4、接收消息</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">~/kafka/bin/kafka-console-consumer.sh --bootstrap-server 192.168.14.123:49092 --topic <span class="built_in">test</span> --from-beginning</span><br></pre></td></tr></table></figure>

<p>展示上面输入的两个消息，表示成功;</p>
<ul>
<li><code>--from-beginning</code> 是从开始的消息展示</li>
</ul>
<h3 id="5、查看特定主题的详细信息"><a href="#5、查看特定主题的详细信息" class="headerlink" title="5、查看特定主题的详细信息"></a>5、查看特定主题的详细信息</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">~/kafka/bin/kafka-topics.sh --bootstrap-server 192.168.14.123:49092 --describe  --topic <span class="built_in">test</span></span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Topic: <span class="built_in">test</span>	PartitionCount: 1	ReplicationFactor: 1	Configs: </span><br><span class="line">	Topic: <span class="built_in">test</span>	Partition: 0	Leader: 0	Replicas: 0	Isr: 0</span><br></pre></td></tr></table></figure>



<ul>
<li>“leader”是负责给定分区所有读写操作的节点。每个节点都是随机选择的部分分区的领导者。</li>
<li>“replicas”是复制分区日志的节点列表，不管这些节点是leader还是仅仅活着。</li>
<li>“isr”是一组“同步”replicas，是replicas列表的子集，它活着并被指到leader。</li>
</ul>
<h3 id="6、删除主题"><a href="#6、删除主题" class="headerlink" title="6、删除主题"></a>6、删除主题</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">~/kafka/bin/kafka-topics.sh --bootstrap-server 192.168.14.123:49092 --delete  --topic <span class="built_in">test</span></span><br><span class="line">~/kafka/bin/kafka-topics.sh --list --bootstrap-server 192.168.14.123:49092</span><br></pre></td></tr></table></figure>

<h2 id="六、设置多个broker集群"><a href="#六、设置多个broker集群" class="headerlink" title="六、设置多个broker集群"></a>六、设置多个<code>broker</code>集群</h2><p>一个 <code>broker</code>只是集群中的一个节点，<code>id</code>是这个节点的名称。</p>
<h3 id="1、创建多个节点的配置"><a href="#1、创建多个节点的配置" class="headerlink" title="1、创建多个节点的配置"></a>1、创建多个节点的配置</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cp</span> ~/kafka/config/server.properties ~/kafka/config/server-1.properties </span><br><span class="line"><span class="built_in">cp</span> ~/kafka/config/server.properties ~/kafka/config/server-2.properties</span><br></pre></td></tr></table></figure>

<p>修改每个配置的 <code>id</code>，<code>id</code> 从0开始。</p>
<p>简单的配置如下</p>
<p><code>server-1.properties</code></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 修改节点名称</span></span><br><span class="line">broker.id=1 </span><br><span class="line"><span class="comment"># 监听的地址和端口，当前监听本机所有</span></span><br><span class="line">listeners=PLAINTEXT://0.0.0.0:49093</span><br><span class="line"><span class="comment"># 主机地址</span></span><br><span class="line">advertised.listeners=PLAINTEXT://192.168.14.123:49093</span><br><span class="line"><span class="comment"># 修改日志目录，防止覆盖日志</span></span><br><span class="line">log.dir=/tmp/kafka-logs-1</span><br><span class="line"><span class="comment"># 配置 Zookeeper</span></span><br><span class="line">zookeeper.connect=192.168.14.123:4181</span><br></pre></td></tr></table></figure>

<p><code>server-2.properties</code></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 修改节点名称</span></span><br><span class="line">broker.id=2 </span><br><span class="line"><span class="comment"># 监听的地址和端口，当前监听本机所有</span></span><br><span class="line">listeners=PLAINTEXT://0.0.0.0:49094</span><br><span class="line"><span class="comment"># 主机地址</span></span><br><span class="line">advertised.listeners=PLAINTEXT://192.168.14.123:49094</span><br><span class="line"><span class="comment"># 修改日志目录，防止覆盖日志</span></span><br><span class="line">log.dir=/tmp/kafka-logs-2</span><br><span class="line"><span class="comment"># 配置 Zookeeper</span></span><br><span class="line">zookeeper.connect=192.168.14.123:4181</span><br></pre></td></tr></table></figure>

<h3 id="2、启动其它配置的服务"><a href="#2、启动其它配置的服务" class="headerlink" title="2、启动其它配置的服务"></a>2、启动其它配置的服务</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">~/kafka/bin/kafka-server-start.sh -daemon ~/kafka/config/server-1.properties</span><br><span class="line">~/kafka/bin/kafka-server-start.sh -daemon ~/kafka/config/server-2.properties</span><br></pre></td></tr></table></figure>

<h3 id="3、创建新的-topic"><a href="#3、创建新的-topic" class="headerlink" title="3、创建新的 topic"></a>3、创建新的 topic</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">~/kafka/bin/kafka-topics.sh --create --bootstrap-server 192.168.14.123:49092 --replication-factor 3 --partitions 1 --topic my-replicated-3-topic </span><br><span class="line">~/kafka/bin/kafka-topics.sh --list --bootstrap-server 192.168.14.123:49092</span><br></pre></td></tr></table></figure>

<p>成功提示：&#96;Created topic my-replicated-3-topic.</p>
<p>这时候已经完成了一个集群的配置。</p>
<blockquote>
<p>启动失败解决</p>
<p>如果报 <code>replication</code> 不够的错误，则是因为 <code>server1</code>、<code>server2</code>可能启动失败了。</p>
<p>可以把 <code>-daemon</code>参数删掉，然后执行命令查看错误 <code>bin/kafka-server-start.sh config/server-1.properties</code>;</p>
<p>可以直接查看日志 <code>logs/server.out</code>;</p>
</blockquote>
<h3 id="4、查看主题列表"><a href="#4、查看主题列表" class="headerlink" title="4、查看主题列表"></a>4、查看主题列表</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">~/kafka/bin/kafka-topics.sh --bootstrap-server 192.168.14.123:49092 --describe  --topic my-replicated-3-topic </span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Topic: my-replicated-3-topic	PartitionCount: 1	ReplicationFactor: 3	Configs: </span><br><span class="line">	Topic: my-replicated-3-topic	Partition: 0	Leader: 1	Replicas: 1,0,2	Isr: 1,0,2</span><br></pre></td></tr></table></figure>

<p><code>leader： 1</code>表示当前 <code>leader</code>在节点1上。</p>
<h3 id="5、收发消息测试"><a href="#5、收发消息测试" class="headerlink" title="5、收发消息测试"></a>5、收发消息测试</h3><h4 id="接收消息"><a href="#接收消息" class="headerlink" title="接收消息"></a>接收消息</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">~/kafka/bin/kafka-console-consumer.sh --bootstrap-server 192.168.14.123:49092 --topic my-replicated-3-topic</span><br></pre></td></tr></table></figure>

<h4 id="发送消息"><a href="#发送消息" class="headerlink" title="发送消息"></a>发送消息</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">~/kafka/bin/kafka-console-producer.sh --broker-list 192.168.14.123:49092 --topic my-replicated-3-topic</span><br></pre></td></tr></table></figure>

<h3 id="6、测试集群-leader崩溃"><a href="#6、测试集群-leader崩溃" class="headerlink" title="6、测试集群 leader崩溃"></a>6、测试集群 <code>leader</code>崩溃</h3><h4 id="停止-leader1"><a href="#停止-leader1" class="headerlink" title="停止 leader1"></a>停止 <code>leader1</code></h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查询进程</span></span><br><span class="line">ps -ef|grep server-1.properties</span><br><span class="line"><span class="comment"># 查询 kafka java 进程更方便</span></span><br><span class="line">jps -<span class="built_in">mv</span> |grep server-1.properties</span><br><span class="line"><span class="comment"># 杀掉进程</span></span><br><span class="line"><span class="built_in">kill</span> -9 xxx</span><br></pre></td></tr></table></figure>

<h4 id="查看主题"><a href="#查看主题" class="headerlink" title="查看主题"></a>查看主题</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">~/kafka/bin/kafka-topics.sh --describe --broker-list 192.168.14.123:49092 --topic my-replicated-3-topic</span><br></pre></td></tr></table></figure>

<p>此时的 <code>leader</code>已经变为了 0，所以 <code>broker 0</code> 成为了 <code>leader</code>, <code>broker1</code>已经不在备份集合里了。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Topic: my-replicated-3-topic	PartitionCount: 1	ReplicationFactor: 3	Configs: </span><br><span class="line">	Topic: my-replicated-3-topic	Partition: 0	Leader: 0	Replicas: 1,0,2	Isr: 0,2</span><br></pre></td></tr></table></figure>

<h4 id="发送消息测试"><a href="#发送消息测试" class="headerlink" title="发送消息测试"></a>发送消息测试</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">~/kafka/bin/kafka-console-producer.sh --broker-list 192.168.14.123:49094 --topic my-replicated-3-topic</span><br></pre></td></tr></table></figure>

<h4 id="查询所有消息"><a href="#查询所有消息" class="headerlink" title="查询所有消息"></a>查询所有消息</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">~/kafka/bin/kafka-console-consumer.sh --bootstrap-server 192.168.14.123:49092 --topic my-replicated-3-topic --from-beginning</span><br></pre></td></tr></table></figure>

<p>这个时候会发现所有的消息都没有丢</p>
<h4 id="删除主题"><a href="#删除主题" class="headerlink" title="删除主题"></a>删除主题</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">~/kafka/bin/kafka-topics.sh --bootstrap-server 192.168.14.123:49092 --delete  --topic my-replicated-3-topic</span><br><span class="line">~/kafka/bin/kafka-topics.sh --list --bootstrap-server 192.168.14.123:49092</span><br></pre></td></tr></table></figure>

<h2 id="七、安全"><a href="#七、安全" class="headerlink" title="七、安全"></a>七、安全</h2><p><a target="_blank" rel="noopener" href="https://kafka.apachecn.org/documentation.html#security_ssl">官方说明</a></p>
<h2 id="Zookeeper配置"><a href="#Zookeeper配置" class="headerlink" title="Zookeeper配置"></a>Zookeeper配置</h2><p>当前下载的kafka程序里自带Zookeeper，可以直接使用其自带的Zookeeper建立集群，也可以单独使用Zookeeper安装文件建立集群。</p>
<h3 id="1-单独使用Zookeeper安装文件建立集群"><a href="#1-单独使用Zookeeper安装文件建立集群" class="headerlink" title="1. 单独使用Zookeeper安装文件建立集群"></a>1. 单独使用Zookeeper安装文件建立集群</h3><p>Zookeeper的安装及配置可以参考另一篇博客，里面有详细介绍</p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/zhaoshizi/p/12105143.html">ZooKeeper的安装和配置过程</a></p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/zhaoshizi/p/12105143.html">https://www.cnblogs.com/zhaoshizi/p/12105143.html</a></p>
<h2 id="Kafka与Zookeeper的关系"><a href="#Kafka与Zookeeper的关系" class="headerlink" title="Kafka与Zookeeper的关系"></a>Kafka与Zookeeper的关系</h2><p>一个典型的Kafka集群中包含若干Produce，若干broker（一般broker数量越多，集群吞吐率越高），若干Consumer Group，以及一个Zookeeper集群。Kafka通过Zookeeper管理集群配置，选举leader，以及在Consumer Group发生变化时进行rebalance。Producer使用push模式将消息发布到broker，Consumer使用pull模式从broker订阅并消费消息。<br><img src="https://img-blog.csdnimg.cn/img_convert/38eea86ae43f4e40666d65f33fe590c0.png" alt="img"><br>1)Producer端直接连接broker.list列表,从列表中返回TopicMetadataResponse,该Metadata包含Topic下每个partition leader建立socket连接并发送消息.</p>
<p>2)Broker端使用zookeeper用来注册broker信息,以及监控partition leader存活性.</p>
<p>3)Consumer端使用zookeeper用来注册consumer信息,其中包括consumer消费的partition列表等,同时也用来发现broker列表,并和partition leader建立socket连接,并获取消息。</p>
<p><strong>Zookeeper作用：管理broker、consumer</strong></p>
<p>创建Broker后，向zookeeper注册新的broker信息，实现在服务器正常运行下的水平拓展。具体的，通过注册watcher，获取partition的信息。</p>
<p>Topic的注册，zookeeper会维护topic与broker的关系，通&#x2F;brokers&#x2F;topics&#x2F;topic.name节点来记录。</p>
<p>Producer向zookeeper中注册watcher,了解topic的partition的消息，以动态了解运行情况，实现负载均衡。Zookeepr不管理producer，只是能够提供当前broker的相关信息。</p>
<p>Consumer可以使用group形式消费kafka中的数据。所有的group将以轮询的方式消费broker中的数据，具体的按照启动的顺序。Zookeeper会给每个consumer group一个ID,即同一份数据可以被不同的用户ID多次消费。因此这就是单播与多播的实现。以单个消费者还是以组别的方式去消费数据，由用户自己去定义。Zookeeper管理consumer的offset跟踪当前消费的offset。</p>
<p>kafka使用ZooKeeper用于管理、协调代理。每个Kafka代理通过Zookeeper协调其他Kafka代理。<br>当Kafka系统中新增了代理或某个代理失效时，Zookeeper服务将通知生产者和消费者。生产者与消费者据此开始与其他代理协调工作。</p>
<p><strong>Zookeeper在Kakfa中扮演的角色：Kafka将元数据信息保存在Zookeeper中，但是发送给Topic本身的数据是不会发到Zk上的</strong></p>
<p>· kafka使用zookeeper来实现动态的集群扩展，不需要更改客户端（producer和consumer）的配置。broker会在zookeeper注册并保持相关的元数据（topic，partition信息等）更新。<br>· 而客户端会在zookeeper上注册相关的watcher。一旦zookeeper发生变化，客户端能及时感知并作出相应调整。这样就保证了添加或去除 broker时，各broker间仍能自动实现负载均衡。这里的客户端指的是Kafka的消息生产端(Producer)和消息消费端(Consumer)<br>· Broker端使用zookeeper来注册broker信息,以及监测partitionleader存活性.<br>· Consumer端使用zookeeper用来注册consumer信息,其中包括consumer消费的partition列表等,同时也用来发现broker列表,并和partitionleader建立socket连接,并获取消息.<br>· Zookeer和Producer没有建立关系，只和Brokers、Consumers建立关系以实现负载均衡，即同一个ConsumerGroup中的Consumers可以实现负载均衡（因为Producer是瞬态的，可以发送后关闭，无需直接等待）</p>
<p>本文地址： <a href="https://github.com/maxzhao-it/blog/post/4175/">https://github.com/maxzhao-it/blog/post/4175/</a> </p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://github.com/maxzhao-it/blog/post/34489/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/blog/uploads/images/avatar.jpg">
      <meta itemprop="name" content="赵联胜">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="maxzhao">
      <meta itemprop="description" content="小码农赵联胜的博客,从遇到Java到爱上Java，工作之余学习Java生态圈中的各种技术。">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | maxzhao">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/blog/post/34489/" class="post-title-link" itemprop="url">KafkaConnect导入导出数据</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2022-07-04 14:50:22" itemprop="dateModified" datetime="2022-07-04T14:50:22+08:00">2022-07-04</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/blog/categories/DevelopTools/" itemprop="url" rel="index"><span itemprop="name">DevelopTools</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/blog/categories/DevelopTools/kafka/" itemprop="url" rel="index"><span itemprop="name">kafka</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="一、Kafka-Connect-导入-x2F-导出-数据"><a href="#一、Kafka-Connect-导入-x2F-导出-数据" class="headerlink" title="一、Kafka Connect 导入&#x2F;导出 数据"></a>一、Kafka Connect 导入&#x2F;导出 数据</h2><p>从控制台写入和写回数据是一个方便的开始，但你可能想要从其他来源导入或导出数据到其他系统。对于大多数系统，可以使用<code>kafka Connect</code>，而不需要编写自定义集成代码。</p>
<p><code>Kafka Connect</code>是导入和导出数据的一个工具。是一个在 <code>kafka</code>与其他系统之间的可扩展和可靠的流数据工具。它使得快速定义连接器变得非常简单，这些连接器可以将大量数据移入和移出<code>Kafka</code>。<code>Kafka Connect</code><br>可以摄取整个数据库或者从你所有的应用服务器上收集指标到Kafka主题中，使数据可以用于低延迟的流处理。导出作业可以将数据从<code>Kafka</code>主题交付到二级存储和查询系统，或者交付到批处理系统以进行离线分析。</p>
<h3 id="Kafka-Connect功能包括"><a href="#Kafka-Connect功能包括" class="headerlink" title="Kafka Connect功能包括:"></a>Kafka Connect功能包括:</h3><ul>
<li>Kafka Connect是Kafka连接器的通用框架——Kafka Connect标准化了其他数据系统与Kafka的集成，简化了连接器的开发、部署和管理</li>
<li>分布式和独立模式——向上扩展到支持整个组织的大型集中管理服务，或者向下扩展到开发、测试和小型生产部署</li>
<li>通过一个易于使用的REST API向Kafka Connect集群提交和管理连接器</li>
<li>自动偏移量管理——Kafka Connect可以自动管理偏移量提交过程，所以连接器开发人员不需要担心连接器开发中这个容易出错的部分</li>
<li>Kafka Connect建立在现有的组管理协议上。可以添加更多的worker来扩展Kafka Connect集群。</li>
<li>流&#x2F;批处理集成-利用Kafka现有的功能，Kafka Connect是一个理想的解决方案桥接流和批处理数据系统</li>
</ul>
<p>一些概念</p>
<ul>
<li><p><code>kafka connector</code>:是<code>kafka connect</code>的关键组成部分，它是一个逻辑上的job,用于在kafka和其他系统之间拷贝数据,比如：从上游系统拷贝数据到kafka,或者从kafka拷贝数据到下游系统</p>
</li>
<li><p>Tasks:每个<code>kafka connector</code>可以初始化一组task进行数据的拷贝</p>
</li>
<li><p>Workers:逻辑上包含<code>kafka connector</code>和tasks用来调度执行具体任务的进程,具体执行时分为standalone模式和distributed模式</p>
</li>
</ul>
<p>​</p>
<p>下面介绍配置、运行和管理Kafka Connect。</p>
<h2 id="二、运行-Kafka-Connect"><a href="#二、运行-Kafka-Connect" class="headerlink" title="二、运行 Kafka Connect"></a>二、运行 Kafka Connect</h2><p>Kafka Connect目前支持两种执行模式:单机(单进程)和分布式。</p>
<p>在独立模式下，所有工作都在单个进程中执行。这种配置更容易设置和开始，可能在只有一个worker有意义的情况下有用(例如收集日志文件)，但它没有从Kafka Connect的一些特性中受益，比如容错。可以使用如下命令启动独立进程:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/connect-standalone.sh config/connect-standalone.properties connector1.properties [connector2.properties ...]</span><br></pre></td></tr></table></figure>

<h3 id="单例配置"><a href="#单例配置" class="headerlink" title="单例配置"></a>单例配置</h3><p>配置文件在 <code>config/connect-standalone.properties</code></p>
<p>首先是Kafka Connect处理的配置，包含常见的配置，例如要连接的Kafka broker和数据的序列化格式。其余的配置文件都指定了要创建的连接器。包括连接器唯一名称，和要实例化的连接器类。以及连接器所需的任何其他配置。</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 连接器的唯一名称。再次尝试注册相同名称将会失败。</span></span><br><span class="line"><span class="attr">nname</span></span><br><span class="line"><span class="comment"># kafka 服务器地址</span></span><br><span class="line"><span class="attr">bootstrap.servers</span>=<span class="string">localhost:9092</span></span><br><span class="line"><span class="comment"># 唯一的字符串，用于标识此worker所属的Connect集群组。</span></span><br><span class="line"><span class="comment"># group.id=</span></span><br><span class="line"><span class="comment"># 用于在Kafka连接格式和写入Kafka的序列化格式之间转换的转换器类。</span></span><br><span class="line"><span class="comment"># 控制了从Kafka写入或读取的消息中的键的格式，并且由于它独立于连接器，它允许任何连接器使用任何序列化格式。常见的格式包括JSON和Avro。</span></span><br><span class="line"><span class="attr">key.converter</span>=<span class="string">org.apache.kafka.connect.json.JsonConverter</span></span><br><span class="line"><span class="comment"># 控制了写入或从Kafka读取的消息中的值的格式，并且由于它独立于连接器，它允许任何连接器使用任何序列化格式。常见的格式包括JSON和Avro。</span></span><br><span class="line"><span class="attr">value.converter</span>=<span class="string">org.apache.kafka.connect.json.JsonConverter</span></span><br><span class="line"><span class="comment"># Converter-specific settings can be passed in by prefixing the Converter&#x27;s setting with the converter we want to apply</span></span><br><span class="line"><span class="comment"># it to</span></span><br><span class="line"><span class="attr">key.converter.schemas.enable</span>=<span class="string">true</span></span><br><span class="line"><span class="attr">value.converter.schemas.enable</span>=<span class="string">true</span></span><br><span class="line"><span class="comment"># 用逗号分隔的主题列表，用作此连接器的输入的话题</span></span><br><span class="line"><span class="comment"># topics</span></span><br><span class="line"><span class="comment"># 主题的Java正则表达式，用作此连接器的输入对于任何其他选项，您应该查阅连接器的文档。</span></span><br><span class="line"><span class="comment"># topics.regex=</span></span><br><span class="line"><span class="comment"># kafka topic仓库配置,需要手动创建主题，以确保正确配置 default connect-configs</span></span><br><span class="line"><span class="comment"># config.storage.topic</span></span><br><span class="line"><span class="attr">offset.storage.file.filename</span>=<span class="string">/tmp/connect.offsets</span></span><br><span class="line"><span class="comment"># Flush much faster than normal, which is useful for testing/debugging</span></span><br><span class="line"><span class="attr">offset.flush.interval.ms</span>=<span class="string">10000</span></span><br></pre></td></tr></table></figure>

<h3 id="控制台操作"><a href="#控制台操作" class="headerlink" title="控制台操作"></a>控制台操作</h3><p>在这个快速入门里，我们将看到如何运行<code>Kafka Connect</code>用简单的连接器从文件导入数据到Kafka主题，再从<code>Kafka</code>主题导出数据到文件。</p>
<p>首先，我们首先创建一些种子数据用来测试：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> -e <span class="string">&quot;foo\nbar&quot;</span> &gt; test.txt</span><br></pre></td></tr></table></figure>

<p>接下来，我们开始2个连接器运行在独立的模式，这意味着它们运行在一个单一的，本地的，专用的进程。我们提供3个配置文件作为参数。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">bin/connect-standalone.sh config/connect-standalone.properties config/connect-file-source.properties config/connect-file-sink.properties</span><br><span class="line"><span class="comment"># 这是分布式的命令，暂时不考虑</span></span><br><span class="line">bin/connect-distributed.sh config/connect-distributed.properties</span><br></pre></td></tr></table></figure>

<p>这些包含在Kafka中的示例配置文件使用之前启动的默认本地群集配置，并创建两个连接器： 第一个是源连接器，用于从输入文件读取行，并将其输入到 Kafka topic。 第二个是接收器连接器，它从Kafka<br>topic中读取消息，并在输出文件中生成一行。</p>
<p>在启动过程中，你会看到一些日志消息，包括一些连接器正在实例化的指示。 一旦Kafka Connect进程启动，源连接器就开始从<code>test.txt</code>读取行并且 将它们生产到主题<code>connect-test</code><br>中，同时接收器连接器也开始从主题<code>connect-test</code>中读取消息， 并将它们写入文件<code>test.sink.txt</code>中。我们可以通过检查输出文件的内容来验证数据是否已通过整个pipeline进行交付：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">more test.sink.txt</span><br></pre></td></tr></table></figure>

<p>出现结果</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">foo</span><br><span class="line">bar</span><br></pre></td></tr></table></figure>

<p>请注意，数据存储在Kafka topic<code>connect-test</code>中，因此我们也可以运行一个console consumer（控制台消费者）来查看 topic 中的数据（或使用custom<br>consumer（自定义消费者）代码进行处理）：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic connect-test --from-beginning</span><br></pre></td></tr></table></figure>

<p>结果：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">&quot;schema&quot;</span>:&#123;<span class="string">&quot;type&quot;</span>:<span class="string">&quot;string&quot;</span>,<span class="string">&quot;optional&quot;</span>:<span class="literal">false</span>&#125;,<span class="string">&quot;payload&quot;</span>:<span class="string">&quot;foo&quot;</span>&#125;</span><br><span class="line">&#123;<span class="string">&quot;schema&quot;</span>:&#123;<span class="string">&quot;type&quot;</span>:<span class="string">&quot;string&quot;</span>,<span class="string">&quot;optional&quot;</span>:<span class="literal">false</span>&#125;,<span class="string">&quot;payload&quot;</span>:<span class="string">&quot;bar&quot;</span>&#125;</span><br></pre></td></tr></table></figure>

<p>连接器一直在处理数据，所以我们可以将数据添加到文件中，并看到它在pipeline 中移动：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 追加一行数据</span></span><br><span class="line"><span class="built_in">echo</span> Another line &gt;&gt; test.txt</span><br></pre></td></tr></table></figure>

<p>应该可以看到这一行出现在控制台用户输出和接收器文件中。</p>
<h2 id="三、MySql-操作"><a href="#三、MySql-操作" class="headerlink" title="三、MySql 操作"></a>三、<code>MySql</code> 操作</h2><p><a target="_blank" rel="noopener" href="https://www.pianshen.com/article/3877182847/">https://www.pianshen.com/article/3877182847/</a></p>
<h2 id="四、详细配置"><a href="#四、详细配置" class="headerlink" title="四、详细配置"></a>四、详细配置</h2><table>
<thead>
<tr>
<th>NAME</th>
<th>DESCRIPTION</th>
<th>TYPE</th>
<th>DEFAULT</th>
<th>VALID VALUES</th>
<th>IMPORTANCE</th>
</tr>
</thead>
<tbody><tr>
<td>config.storage.topic</td>
<td>kafka topic仓库配置</td>
<td>string</td>
<td></td>
<td></td>
<td>high</td>
</tr>
<tr>
<td>group.id</td>
<td>唯一的字符串，用于标识此worker所属的Connect集群组。</td>
<td>string</td>
<td></td>
<td></td>
<td>high</td>
</tr>
<tr>
<td>key.converter</td>
<td>用于Kafka Connect和写入到Kafka的序列化消息的之间格式转换的转换器类。 这可以控制写入或从kafka读取的消息中的键的格式，并且由于这与连接器无关，因此它允许任何连接器使用任何序列化格式。 常见格式的示例包括JSON和Avro。</td>
<td>class</td>
<td></td>
<td></td>
<td>high</td>
</tr>
<tr>
<td>offset.storage.topic</td>
<td>连接器的offset存储到哪个topic中</td>
<td>string</td>
<td></td>
<td></td>
<td>high</td>
</tr>
<tr>
<td>status.storage.topic</td>
<td>追踪连接器和任务状态存储到哪个topic中</td>
<td>string</td>
<td></td>
<td></td>
<td>high</td>
</tr>
<tr>
<td>value.converter</td>
<td>用于Kafka Connect格式和写入Kafka的序列化格式之间转换的转换器类。 控制了写入或从Kafka读取的消息中的值的格式，并且由于这与连接器无关，因此它允许任何连接器使用任何序列化格式。 常见格式的示例包括JSON和Avro。</td>
<td>class</td>
<td></td>
<td></td>
<td>high</td>
</tr>
<tr>
<td>internal.key.converter</td>
<td>用于在Kafka Connect格式和写入Kafka的序列化格式之间转换的转换器类。 这可以控制写入或从Kafka读取的消息中的key的格式，并且由于这与连接器无关，因此它允许任何连接器使用任何序列化格式。 常见格式的示例包括JSON和Avro。 此设置用于控制框架内部使用的记账数据的格式，例如配置和偏移量，因此用户可以使用运行各种Converter实现。</td>
<td>class</td>
<td></td>
<td></td>
<td>low</td>
</tr>
<tr>
<td>internal.value.converter</td>
<td>用于在Kafka Connect格式和写入Kafka的序列化格式之间转换的转换器类。 这控制了写入或从Kafka读取的消息中的值的格式，并且由于这与连接器无关，因此它允许任何连接器使用任何序列化格式。 常见格式的示例包括JSON和Avro。 此设置用于控制框架内部使用的记账数据的格式，例如配置和偏移量，因此用户可以使用运行各种Converter实现。</td>
<td>class</td>
<td></td>
<td></td>
<td>low</td>
</tr>
<tr>
<td>bootstrap.servers</td>
<td>用于建立与Kafka集群的初始连接的主机&#x2F;端口列表。此列表用来发现完整服务器集的初始主机。 该列表的格式应为host1：port1，host2：port2，….由于这些服务器仅用于初始连接以发现完整的集群成员资格（可能会动态更改），因此,不需要包含完整的服务器（尽管如此，你需要多配置几个，以防止配置的宕机）。</td>
<td>list</td>
<td>localhost:9092</td>
<td></td>
<td>high</td>
</tr>
<tr>
<td>heartbeat.interval.ms</td>
<td>心跳间隔时间。心跳用于确保会话保持活动，并在新成员加入或离开组时进行重新平衡。 该值必须设置为低于session.timeout.ms，但通常应设置为不高于该值的1&#x2F;3。</td>
<td>int</td>
<td>3000</td>
<td></td>
<td>high</td>
</tr>
<tr>
<td>rebalance.timeout.ms</td>
<td>限制所有组中消费者的任务处理数据和提交offset所需的时间。如果超时，那么woker将从组中删除，这也将导致offset提交失败。</td>
<td>int</td>
<td>60000</td>
<td></td>
<td>high</td>
</tr>
<tr>
<td>session.timeout.ms</td>
<td>用于察觉worker故障的超时时间。worker定时发送心跳以表明自己是活着的。如果broker在会话超时时间到期之前没有接收到心跳，那么broker将从分组中移除该worker，并启动重新平衡。注意，该值必须在<code>group.min.session.timeout.ms</code>和<code>group.max.session.timeout.ms</code>范围内。</td>
<td>int</td>
<td>10000</td>
<td></td>
<td>high</td>
</tr>
<tr>
<td>ssl.key.password</td>
<td>密钥存储文件中私钥的密码。 这对于客户端是可选的。</td>
<td>password</td>
<td>null</td>
<td></td>
<td>high</td>
</tr>
<tr>
<td>ssl.keystore.location</td>
<td>密钥存储文件的位置。 这对于客户端是可选的，可以用于客户端的双向身份验证。</td>
<td>string</td>
<td>null</td>
<td></td>
<td>high</td>
</tr>
<tr>
<td>ssl.keystore.password</td>
<td>密钥存储文件的存储密码。 客户端是可选的，只有配置了ssl.keystore.location才需要。</td>
<td>password</td>
<td>null</td>
<td></td>
<td>high</td>
</tr>
<tr>
<td>ssl.truststore.location</td>
<td>信任存储文件的位置。</td>
<td>string</td>
<td>null</td>
<td></td>
<td>high</td>
</tr>
<tr>
<td>ssl.truststore.password</td>
<td>信任存储文件的密码。</td>
<td>password</td>
<td>null</td>
<td></td>
<td>high</td>
</tr>
<tr>
<td>connections.max.idle.ms</td>
<td>多少毫秒之后关闭空闲的连接。</td>
<td>long</td>
<td>540000</td>
<td></td>
<td>medium</td>
</tr>
<tr>
<td>receive.buffer.bytes</td>
<td>读取数据时使用的TCP接收缓冲区（SO_RCVBUF）的大小。 如果值为-1，则将使用OS默认值。</td>
<td>int</td>
<td>32768</td>
<td>[0,…]</td>
<td>medium</td>
</tr>
<tr>
<td>request.timeout.ms</td>
<td>配置控制客户端等待请求响应的最长时间。 如果在超时之前未收到响应，客户端将在必要时重新发送请求，如果重试耗尽，则该请求将失败。</td>
<td>int</td>
<td>40000</td>
<td>[0,…]</td>
<td>medium</td>
</tr>
<tr>
<td>sasl.jaas.config</td>
<td>用于JAAS配置文件的SASL连接的JAAS登录上下文参数格式。<a target="_blank" rel="noopener" href="https://blog.csdn.net/zqqcometo/article/details/85089475#http://docs.oracle.com/javase/8/docs/technotes/guides/security/jgss/tutorials/LoginConfigFile.html">这里</a>描述了JAAS配置文件的格式。该值的格式为：’ (&#x3D;)*;’</td>
<td>password</td>
<td>null</td>
<td></td>
<td>medium</td>
</tr>
<tr>
<td>sasl.kerberos.service.name</td>
<td>Kafka运行的Kerberos principal名称。 可以在Kafka的JAAS配置或Kafka的配置中定义。</td>
<td>string</td>
<td>null</td>
<td></td>
<td>medium</td>
</tr>
<tr>
<td>sasl.mechanism</td>
<td>用户客户端连接的SASL机制。可以提供者任何安全机制。 GSSAPI是默认机制。</td>
<td>string</td>
<td>GSSAPI</td>
<td></td>
<td>medium</td>
</tr>
<tr>
<td>security.protocol</td>
<td>用于和broker通讯的策略。有效的值有：PLAINTEXT, SSL, SASL_PLAINTEXT, SASL_SSL。</td>
<td></td>
<td>string</td>
<td>PLAINTEXT</td>
<td>medium</td>
</tr>
<tr>
<td>send.buffer.bytes</td>
<td>发送数据时使用TCP发送缓冲区（SO_SNDBUF）的大小。如果值为-1，则将使用OS默认。</td>
<td>int</td>
<td>131072</td>
<td>[-1,…]</td>
<td>medium</td>
</tr>
<tr>
<td>ssl.enabled.protocols</td>
<td>启用SSL连接的协议列表。</td>
<td>list</td>
<td>TLSv1.2,TLSv1 .1,TLSv1</td>
<td></td>
<td>medium</td>
</tr>
<tr>
<td>ssl.keystore.type</td>
<td>密钥存储文件的文件格式。 对于客户端是可选的。</td>
<td>string</td>
<td>JKS</td>
<td></td>
<td>medium</td>
</tr>
<tr>
<td>ssl.protocol</td>
<td>用于生成SSLContext的SSL协议。 默认设置是TLS，这对大多数情况都是适用的。 最新的JVM中的允许值为TLS，TLSv1.1和TLSv1.2。 旧的JVM可能支持SSL，SSLv2和SSLv3，但由于已知的安全漏洞，不建议使用SSL。</td>
<td>string</td>
<td>TLS</td>
<td></td>
<td>medium</td>
</tr>
<tr>
<td>ssl.provider</td>
<td>用于SSL连接的安全提供程序的名称。 默认值是JVM的默认安全提供程序。</td>
<td>string</td>
<td>null</td>
<td></td>
<td>medium</td>
</tr>
<tr>
<td>ssl.truststore.type</td>
<td>信任存储文件的文件格式。</td>
<td>string</td>
<td>JKS</td>
<td></td>
<td>medium</td>
</tr>
<tr>
<td>worker.sync.timeout.ms</td>
<td>当worker与其他worker不同步并需要重新同步配置时，需等待一段时间才能离开组，然后才能重新加入。</td>
<td>int</td>
<td>3000</td>
<td></td>
<td>medium</td>
</tr>
<tr>
<td>worker.unsync.backoff.ms</td>
<td>当worker与其他worker不同步，并且无法在worker.sync.timeout.ms 期间追赶上，在重新连接之前，退出Connect集群的时间。</td>
<td>int</td>
<td>300000</td>
<td></td>
<td>medium</td>
</tr>
<tr>
<td>access.control.allow.methods</td>
<td>通过设置Access-Control-Allow-Methods标头来设置跨源请求支持的方法。 Access-Control-Allow-Methods标头的默认值允许GET，POST和HEAD的跨源请求。</td>
<td>string</td>
<td>“”</td>
<td></td>
<td>low</td>
</tr>
<tr>
<td>access.control.allow.origin</td>
<td>将Access-Control-Allow-Origin标头设置为REST API请求。要启用跨源访问，请将其设置为应该允许访问API的应用程序的域，或者 *” 以允许从任何的<code>域</code>。 默认值只允许从REST API的域访问。</td>
<td>string</td>
<td>“”</td>
<td></td>
<td>low</td>
</tr>
<tr>
<td>client.id</td>
<td>在发出请求时传递给服务器的id字符串。这样做的目的是通过允许逻辑应用程序名称包含在请求消息中，来跟踪请求来源。而不仅仅是ip&#x2F;port</td>
<td>string</td>
<td>“”</td>
<td></td>
<td>low</td>
</tr>
<tr>
<td>config.storage.replication.factor</td>
<td>当创建配置仓库topic时的副本数</td>
<td>short</td>
<td>3</td>
<td>[1,…]</td>
<td>low</td>
</tr>
<tr>
<td>metadata.max.age.ms</td>
<td>在没有任何分区leader改变，主动地发现新的broker或分区的时间。</td>
<td>long</td>
<td>300000</td>
<td>[0,…]</td>
<td>low</td>
</tr>
<tr>
<td>metric.reporters</td>
<td>A list of classes to use as metrics reporters. Implementing the MetricReporter interface allows plugging in classes that will be notified of new metric creation. The JmxReporter is always included to register JMX statistics.</td>
<td>list</td>
<td>“”</td>
<td></td>
<td>low</td>
</tr>
<tr>
<td>metrics.num.samples</td>
<td>保留计算metrics的样本数（译者不清楚是做什么的）</td>
<td>int</td>
<td>2</td>
<td>[1,…]</td>
<td>low</td>
</tr>
<tr>
<td>metrics.sample.window.ms</td>
<td>The window of time a metrics sample is computed over.</td>
<td>long</td>
<td>30000</td>
<td>[0,…]</td>
<td>low</td>
</tr>
<tr>
<td>offset.flush.interval.ms</td>
<td>尝试提交任务偏移量的间隔。</td>
<td>long</td>
<td>60000</td>
<td></td>
<td>low</td>
</tr>
<tr>
<td>offset.flush.timeout.ms</td>
<td>在取消进程并恢复要在之后尝试提交的offset数据之前，等待消息刷新并分配要提交到offset仓库的offset数据的最大毫秒数。</td>
<td>long</td>
<td>5000</td>
<td></td>
<td>low</td>
</tr>
<tr>
<td>offset.storage.partitions</td>
<td>创建offset仓库topic的分区数</td>
<td>int</td>
<td>25</td>
<td>[1,…]</td>
<td>low</td>
</tr>
<tr>
<td>offset.storage.replication.factor</td>
<td>创建offset仓库topic的副本数</td>
<td>short</td>
<td>3</td>
<td>[1,…]</td>
<td>low</td>
</tr>
<tr>
<td>plugin.path</td>
<td>包含插件(连接器,转换器,转换)逗号(,)分隔的路径列表。该列表应包含顶级目录，其中包括以下任何组合：a）包含jars与插件及其依赖关系的目录 b）具有插件及其依赖项的uber-jars c）包含插件类的包目录结构的目录及其依赖关系,注意配置：将遵循符号链接来发现依赖关系或插件。 示例：plugin.path&#x3D;&#x2F;usr&#x2F;local&#x2F;share&#x2F;java,&#x2F;usr&#x2F;local&#x2F;share&#x2F;kafka&#x2F;plugins,&#x2F;opt&#x2F;connectors</td>
<td>list</td>
<td>null</td>
<td></td>
<td>low</td>
</tr>
<tr>
<td>reconnect.backoff.max.ms</td>
<td>无法连接broker时等待的最大时间（毫秒）。如果设置，则每个host的将会持续的增加，直到达到最大值。计算增加后，再增加20％的随机抖动，以避免高频的反复连接。</td>
<td>long</td>
<td>1000</td>
<td>[0,…]</td>
<td>low</td>
</tr>
<tr>
<td>reconnect.backoff.ms</td>
<td>尝试重新连接到主机之前等待的时间。 避免了高频率反复的连接主机。 这种机制适用于消费者向broker发送的所有请求。</td>
<td>long</td>
<td>50</td>
<td>[0,…]</td>
<td>low</td>
</tr>
<tr>
<td>rest.advertised.host.name</td>
<td>如果设置，其他wokers将通过这个hostname进行连接。</td>
<td>string</td>
<td>null</td>
<td></td>
<td>low</td>
</tr>
<tr>
<td>rest.advertised.port</td>
<td>如果设置，其他的worker将通过这个端口进行连接。</td>
<td>int</td>
<td>null</td>
<td></td>
<td>low</td>
</tr>
<tr>
<td>rest.host.name</td>
<td>REST API的主机名。 如果设置，它将只绑定到这个接口。</td>
<td>string</td>
<td>null</td>
<td></td>
<td>low</td>
</tr>
<tr>
<td>rest.port</td>
<td>用于监听REST API的端口</td>
<td>int</td>
<td>8083</td>
<td>low</td>
<td></td>
</tr>
<tr>
<td>retry.backoff.ms</td>
<td>失败请求重新尝试之前的等待时间，避免了在某些故障的情况下，频繁的重复发送请求。</td>
<td>long</td>
<td>100</td>
<td>[0,…]</td>
<td>low</td>
</tr>
<tr>
<td>sasl.kerberos.kinit.cmd</td>
<td>Kerberos kinit命令路径.</td>
<td>string</td>
<td>&#x2F;usr&#x2F;bin&#x2F;kinit</td>
<td></td>
<td>low</td>
</tr>
<tr>
<td>sasl.kerberos.min.time.before.relogin</td>
<td>尝试refresh之间登录线程的休眠时间.</td>
<td>long</td>
<td>60000</td>
<td></td>
<td>low</td>
</tr>
<tr>
<td>sasl.kerberos.ticket.renew.jitter</td>
<td>添加到更新时间的随机抖动百分比。</td>
<td>double</td>
<td>0.05</td>
<td></td>
<td>low</td>
</tr>
<tr>
<td>sasl.kerberos.ticket.renew.window.factor</td>
<td>登录线程将休眠，直到从上次刷新ticket到期，此时将尝试续订ticket。</td>
<td>double</td>
<td>0.8</td>
<td></td>
<td>low</td>
</tr>
<tr>
<td>ssl.cipher.suites</td>
<td>密码套件列表。用于TLS或SSL网络协议协商网络连接的安全设置的认证，加密，MAC和密钥交换算法的命名组合。 默认情况下，支持所有可用的密码套件。</td>
<td>list</td>
<td>null</td>
<td></td>
<td>low</td>
</tr>
<tr>
<td>ssl.endpoint.identification.algorithm</td>
<td>末端识别算法使用服务器证书验证服务器主机名。</td>
<td>string</td>
<td>null</td>
<td></td>
<td>low</td>
</tr>
<tr>
<td>ssl.keymanager.algorithm</td>
<td>用于SSL连接的key管理工厂的算法，默认值是Java虚拟机配置的密钥管理工厂算法。</td>
<td>string</td>
<td>SunX509</td>
<td></td>
<td>low</td>
</tr>
<tr>
<td>ssl.secure.random.implementation</td>
<td>用于SSL加密操作的SecureRandom PRNG实现。</td>
<td>string</td>
<td>null</td>
<td></td>
<td>low</td>
</tr>
<tr>
<td>ssl.trustmanager.algorithm</td>
<td>用于SSL连接的信任管理仓库算法。默认值是Java虚拟机配置的信任管理器工厂算法。</td>
<td>string</td>
<td>PKIX</td>
<td></td>
<td>low</td>
</tr>
<tr>
<td>status.storage.partitions</td>
<td>用于创建状态仓库topic的分区数</td>
<td>int</td>
<td>5</td>
<td>[1,…]</td>
<td>low</td>
</tr>
<tr>
<td>status.storage.replication.factor</td>
<td>用于创建状态仓库topic的副本数</td>
<td>short</td>
<td>3</td>
<td>[1,…]</td>
<td>low</td>
</tr>
<tr>
<td>task.shutdown.graceful.timeout.ms</td>
<td>等待任务正常关闭的时间，这是总时间，不是每个任务，所有任务触发关闭，然后依次等待。</td>
<td>long</td>
<td>5000</td>
<td></td>
<td>low</td>
</tr>
</tbody></table>
<p>本文地址： <a href="https://github.com/maxzhao-it/blog/post/34489/">https://github.com/maxzhao-it/blog/post/34489/</a> </p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://github.com/maxzhao-it/blog/post/23026/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/blog/uploads/images/avatar.jpg">
      <meta itemprop="name" content="赵联胜">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="maxzhao">
      <meta itemprop="description" content="小码农赵联胜的博客,从遇到Java到爱上Java，工作之余学习Java生态圈中的各种技术。">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | maxzhao">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/blog/post/23026/" class="post-title-link" itemprop="url">Linux删除文件之后恢复（ext3grep）</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2022-07-04 14:50:22" itemprop="dateModified" datetime="2022-07-04T14:50:22+08:00">2022-07-04</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/blog/categories/Linux/" itemprop="url" rel="index"><span itemprop="name">Linux</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>某天，不小心误删了 $HOME 路径下的数据。</p>
<p>网上找到了<code>ext3grep</code>尝试恢复;</p>
<p>安装</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo pacman -S ext3grep</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>查看版本</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ext3grep -v</span><br></pre></td></tr></table></figure>

<p> 输出下面信息</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Running ext3grep version 0.10.2</span><br><span class="line">ext3grep v0.10.2, Copyright (C) 2008 Carlo Wood.</span><br><span class="line">ext3grep comes with ABSOLUTELY NO WARRANTY;</span><br><span class="line">This program is free software; your freedom to use, change</span><br><span class="line">and distribute this program is protected by the GPL.</span><br></pre></td></tr></table></figure>

<p>查看命令</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ext3grep --<span class="built_in">help</span> </span><br></pre></td></tr></table></figure>

<p>查询要恢复文件的 inode 号</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ext3grep /dev/sda5 --ls --inode 10092546</span><br></pre></td></tr></table></figure>

<p>查询最大 <code>inode</code>号</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ls -id</span><br></pre></td></tr></table></figure>



<p>失败</p>
<p>本文地址： <a href="https://github.com/maxzhao-it/blog/post/23026/">https://github.com/maxzhao-it/blog/post/23026/</a> </p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://github.com/maxzhao-it/blog/post/48634/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/blog/uploads/images/avatar.jpg">
      <meta itemprop="name" content="赵联胜">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="maxzhao">
      <meta itemprop="description" content="小码农赵联胜的博客,从遇到Java到爱上Java，工作之余学习Java生态圈中的各种技术。">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | maxzhao">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/blog/post/48634/" class="post-title-link" itemprop="url">ArchLinux安装GRUB主题</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2022-07-04 14:50:22" itemprop="dateModified" datetime="2022-07-04T14:50:22+08:00">2022-07-04</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/blog/categories/Linux/" itemprop="url" rel="index"><span itemprop="name">Linux</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>下载主题到 <a target="_blank" rel="noopener" href="https://link.jianshu.com/?t=https://www.gnome-look.org/browse/cat/109/ord/latest/">gnome-look</a> 。</p>
<p>我这里下载的排名最高的 <a target="_blank" rel="noopener" href="https://www.gnome-look.org/p/1307852/">Tela grub</a></p>
<h3 id="解压缩主题包"><a href="#解压缩主题包" class="headerlink" title="解压缩主题包"></a>解压缩主题包</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo tar -xf 主题包名 </span><br></pre></td></tr></table></figure>

<h3 id="安装主题"><a href="#安装主题" class="headerlink" title="安装主题"></a>安装主题</h3><h4 id="1、复制主题"><a href="#1、复制主题" class="headerlink" title="1、复制主题"></a>1、复制主题</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo <span class="built_in">cp</span> -r 主题包名 /boot/grub/themes/  </span><br><span class="line"><span class="comment"># 或者</span></span><br><span class="line">sudo <span class="built_in">cp</span> -r Tela /usr/share/grub/themes/</span><br></pre></td></tr></table></figure>

<h4 id="2、直接执行install-sh脚本"><a href="#2、直接执行install-sh脚本" class="headerlink" title="2、直接执行install.sh脚本"></a>2、直接执行<code>install.sh</code>脚本</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo sh Tela-2k/install.sh</span><br></pre></td></tr></table></figure>

<h3 id="修改配置文件"><a href="#修改配置文件" class="headerlink" title="修改配置文件"></a>修改配置文件</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo vim /etc/grub.d/00_header</span><br></pre></td></tr></table></figure>

<p>添加如下内容：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">GRUB_THEME=<span class="string">&quot;/boot/grub/themes/主题包名/theme.txt&quot;</span></span><br><span class="line">GRUB_GFXMODE=<span class="string">&quot;1920x1080x32&quot;</span></span><br></pre></td></tr></table></figure>

<h3 id="更新配置文件"><a href="#更新配置文件" class="headerlink" title="更新配置文件"></a>更新配置文件</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo grub-mkconfig -o /boot/grub/grub.cfg</span><br></pre></td></tr></table></figure>

<h3 id="修改启动界面字体"><a href="#修改启动界面字体" class="headerlink" title="修改启动界面字体"></a>修改启动界面字体</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 这里是主题目录</span></span><br><span class="line">vim /usr/share/grub/themes/Tela/theme.txt</span><br></pre></td></tr></table></figure>

<p>修改其中的</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">item_font=<span class="string">&quot;&quot;</span></span><br></pre></td></tr></table></figure>

<p>改好保存就可以了。</p>
<p>本文地址： <a href="https://github.com/maxzhao-it/blog/post/48634/">https://github.com/maxzhao-it/blog/post/48634/</a> </p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/blog/page/29/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/blog/">1</a><span class="space">&hellip;</span><a class="page-number" href="/blog/page/29/">29</a><span class="page-number current">30</span><a class="page-number" href="/blog/page/31/">31</a><span class="space">&hellip;</span><a class="page-number" href="/blog/page/45/">45</a><a class="extend next" rel="next" href="/blog/page/31/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">赵联胜</span>
</div>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/blog/js/comments.js"></script><script src="/blog/js/utils.js"></script><script src="/blog/js/motion.js"></script><script src="/blog/js/next-boot.js"></script>

  
<script src="https://cdn.jsdelivr.net/npm/hexo-generator-searchdb@1.4.0/dist/search.js" integrity="sha256-vXZMYLEqsROAXkEw93GGIvaB2ab+QW6w3+1ahD9nXXA=" crossorigin="anonymous"></script>
<script src="/blog/js/third-party/search/local-search.js"></script>




  <script src="/blog/js/third-party/pace.js"></script>

  





</body>
</html>
